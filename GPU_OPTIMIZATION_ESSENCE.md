# GPU æ€§èƒ½ä¼˜åŒ–æ ¸å¿ƒè¦ä¹‰

> å¦‚ä½•ç”¨æœ€å°‘çš„æ—¶é—´å’Œç²¾åŠ›ï¼Œè·å¾—æœ€å¤§çš„æ€§èƒ½æå‡

## æ ¸å¿ƒæ€æƒ³

**80/20 æ³•åˆ™**:
- 80% çš„æ€§èƒ½æå‡æ¥è‡ª 20% çš„ä¼˜åŒ–æŠ€æœ¯
- 20% çš„æ—¶é—´å¯ä»¥è¾¾åˆ° 80% çš„ä¼˜åŒ–æ•ˆæœ

**å…³é”®**: å¿«é€Ÿè¯Šæ–­ â†’ ç²¾å‡†æ‰“å‡» â†’ åŠæ—¶åœæ‰‹

---

## ä¸€ã€å¿«é€Ÿè¯Šæ–­ (5 åˆ†é’Ÿ)

### åªçœ‹ 2 ä¸ªå…³é”®æŒ‡æ ‡

#### 1.1 Speed of Light (SM% vs Memory%)

**ä½ç½®**: NCU-UI â†’ "GPU Speed Of Light Throughput" é¡µé¢

**åˆ¤æ–­çŸ©é˜µ**:

| Memory% | SM% | ç“¶é¢ˆç±»å‹ | ä¼˜åŒ–æ–¹å‘ |
|---------|-----|----------|----------|
| > 60% | < 40% | **Memory-bound** | ä¼˜åŒ–å†…å­˜è®¿é—® |
| < 40% | > 60% | **Compute-bound** | ä¼˜åŒ–è®¡ç®—æ•ˆç‡ |
| < 40% | < 40% | **Launch-bound** | å¢åŠ å¹¶è¡Œåº¦ |
| > 60% | > 60% | **å·²ä¼˜åŒ–** | è½¬å‘ç³»ç»Ÿçº§ |

**ç¤ºä¾‹**:
```
Memory Throughput: 89.2% ğŸ”´
SM Throughput: 12.3% ğŸŸ¢
â†’ Memory-boundï¼Œé—®é¢˜åœ¨æ˜¾å­˜è®¿é—®
```

#### 1.2 Sectors Per Request

**ä½ç½®**: NCU-UI â†’ "Memory Workload Analysis" â†’ "L1/TEX Cache" è¡¨æ ¼ â†’ `l1tex__average_t_sectors_per_request` è¡Œ

**åˆ¤æ–­**:
- **< 1.5**: âœ… è®¿é—®å·²åˆå¹¶
- **> 1.5**: âŒ è®¿é—®æœªåˆå¹¶ (æµªè´¹å¸¦å®½)

**è®¡ç®—å¸¦å®½æµªè´¹**:
```
å¸¦å®½æµªè´¹ = (sectors_per_request - 1) / sectors_per_request Ã— 100%

ç¤ºä¾‹: sectors_per_request = 32.0
å¸¦å®½æµªè´¹ = (32 - 1) / 32 = 96.9%
```

---

## äºŒã€æ ¸å¿ƒä¼˜åŒ–æŠ€æœ¯ (è§£å†³ 80% é—®é¢˜)

### æŠ€æœ¯ 1: Memory Coalescing (å¿…é¡»æŒæ¡)

**ä½•æ—¶ä½¿ç”¨**: sectors_per_request > 1.5

**æ ¸å¿ƒåŸç†**: ç›¸é‚»çº¿ç¨‹è®¿é—®ç›¸é‚»å†…å­˜

**ç¤ºä¾‹**:
```c
// âŒ éåˆå¹¶ (sectors_per_request = 32)
int stride = 64;
float val = data[tid * stride];  // Thread 0â†’0, Thread 1â†’64, Thread 2â†’128

// âœ… åˆå¹¶ (sectors_per_request = 1.0)
float val = data[tid];  // Thread 0â†’0, Thread 1â†’1, Thread 2â†’2
```

**é¢„æœŸæå‡**: 5-10x
**å¼€å‘æ—¶é—´**: 30 åˆ†é’Ÿ
**ROI**: â­â­â­â­â­

---

### æŠ€æœ¯ 2: Shared Memory Tiling (GEMM ç±»ç®—å­)

**ä½•æ—¶ä½¿ç”¨**:
- sectors_per_request < 1.5 (å·²åˆå¹¶)
- å­˜åœ¨æ•°æ®é‡ç”¨ (å¦‚çŸ©é˜µä¹˜æ³•, å·ç§¯)

**æ ¸å¿ƒåŸç†**: å°†æ•°æ®ç¼“å­˜åˆ° Shared Memoryï¼Œé‡ç”¨å‡å°‘æ˜¾å­˜è®¿é—®

**ä½•æ—¶ NOT ä½¿ç”¨**:
- âŒ Element-wise æ“ä½œ (æ— æ•°æ®é‡ç”¨)
- âŒ æ¯ä¸ªæ•°æ®åªè¯» 1 æ¬¡

**ç¤ºä¾‹ (GEMM)**:
```c
__shared__ float As[32][32+1];  // +1 é¿å… bank conflict
__shared__ float Bs[32][32+1];

// åŠ è½½ tile (åä½œåŠ è½½)
As[ty][tx] = A[...];
Bs[ty][tx] = B[...];
__syncthreads();

// è®¡ç®— (é‡ç”¨ tile æ•°æ®)
for (int k = 0; k < 32; k++) {
    sum += As[ty][k] * Bs[k][tx];  // æ¯ä¸ªå…ƒç´ é‡ç”¨ 32 æ¬¡
}
```

**é¢„æœŸæå‡**: 10-20x (ç›¸æ¯”æœªä½¿ç”¨ Shared Memory)
**å¼€å‘æ—¶é—´**: 2-4 å°æ—¶
**ROI**: â­â­â­â­

---

### æŠ€æœ¯ 3: Tensor Core (æœ€åä¸€è·³)

**ä½•æ—¶ä½¿ç”¨**:
- çŸ©é˜µä¹˜æ³•
- å·²ç»ä¼˜åŒ–åˆ° 20-30% FP32 å³°å€¼
- æƒ³è¦ 10x+ æå‡

**æ ¸å¿ƒåŸç†**: ä½¿ç”¨ä¸“ç”¨ç¡¬ä»¶åŠ é€Ÿ

**æœ€ç®€å•æ–¹æ³•**: ç›´æ¥ç”¨ cuBLAS
```c
cublasGemmEx(handle, ..., CUBLAS_GEMM_DEFAULT_TENSOR_OP);
// è‡ªåŠ¨ä½¿ç”¨ Tensor Core
```

**é¢„æœŸæå‡**: 10-16x (FP32 â†’ FP16 Tensor Core)
**å¼€å‘æ—¶é—´**: 5 åˆ†é’Ÿ (ç”¨ cuBLAS) æˆ– 1-2 å¤© (æ‰‹å†™ WMMA)
**ROI**: â­â­â­â­â­ (ç”¨åº“) æˆ– â­â­ (æ‰‹å†™)

---

## ä¸‰ã€å†³ç­–æ ‘ (ä½•æ—¶åœæ­¢ä¼˜åŒ–)

```
å¼€å§‹
  â†“
è¿è¡Œ NCU (5 åˆ†é’Ÿ)
  â†“
Memory% > 60%?  â”€â”€â”€â”€Noâ”€â”€â†’ Compute% > 60%? â”€â”€â”€â”€Noâ”€â”€â†’ å¢åŠ å¹¶è¡Œåº¦
  â†“ Yes                      â†“ Yes
sectors > 1.5?              ä½¿ç”¨ Tensor Core
  â†“ Yes     â†“ No              (cuBLAS)
ä¿®å¤       æœ‰æ•°æ®é‡ç”¨?
Coalescing   â†“ Yes  â†“ No
(8x)      Shared   å·²è¾¾æé™
          Memory   (æ•ˆç‡ > 80%)
          (15x)      â†“
            â†“      åœæ­¢ä¼˜åŒ– kernel
          éªŒè¯æ•ˆç‡   è½¬å‘ç³»ç»Ÿçº§:
          > 80%?    - ç®—å­èåˆ
            â†“ Yes   - Pipeline
          åœæ­¢      - å¤š GPU
```

### åœæ­¢ä¼˜åŒ–çš„ä¿¡å·

**ç«‹å³åœæ­¢** (è½¬å‘å…¶ä»–æ–¹å‘):
1. å¸¦å®½æ•ˆç‡ > 90%
2. æ€§èƒ½è¾¾åˆ° cuBLAS çš„ 60%+
3. ç»§ç»­ä¼˜åŒ–é¢„æœŸ < 1.5x ä¸”éœ€è¦ > 2 å¤©

**è½¬å‘æ–¹å‘**:
- ç®—å­èåˆ (å‡å°‘ kernel launch)
- ä½¿ç”¨å®˜æ–¹åº“ (cuBLAS, cuDNN)
- ç³»ç»Ÿçº§ä¼˜åŒ– (å¤š GPU, Pipeline)

---

## å››ã€ROI è®¡ç®—

### å…¬å¼

```python
def should_optimize(current_ms, expected_speedup, dev_days):
    """
    current_ms: å½“å‰è€—æ—¶
    expected_speedup: é¢„æœŸåŠ é€Ÿæ¯”
    dev_days: é¢„è®¡å¼€å‘å¤©æ•°
    """
    new_ms = current_ms / expected_speedup
    time_saved = current_ms - new_ms

    # å‡è®¾æ¯å¤©è¿è¡Œ 100 ä¸‡æ¬¡
    daily_saved_hours = (time_saved * 1e6) / 1000 / 3600

    roi = daily_saved_hours / dev_days

    if roi > 1.0:
        return "â­â­â­â­â­ æé«˜ ROIï¼Œç«‹å³æ‰§è¡Œ"
    elif roi > 0.5:
        return "â­â­â­ ä¸­ç­‰ ROIï¼Œå€¼å¾—åš"
    else:
        return "âŒ ROI å¤ªä½ï¼Œä¸å»ºè®®"
```

### å®é™…æ¡ˆä¾‹

**æ¡ˆä¾‹ 1: ä¿®å¤ Coalescing**
```
å½“å‰: 10 ms
é¢„æœŸ: 8x â†’ 1.25 ms
å¼€å‘: 0.5 å¤©
ROI = (8.75 ms Ã— 1M) / 1000 / 3600 / 0.5 = 4.86 å°æ—¶/å¤©
ç»“è®º: â­â­â­â­â­ ç«‹å³æ‰§è¡Œ
```

**æ¡ˆä¾‹ 2: ç»§ç»­ä¼˜åŒ–å·²ç»å¾ˆå¿«çš„ kernel**
```
å½“å‰: 0.5 ms (å·²ç» 85% æ•ˆç‡)
é¢„æœŸ: 1.3x â†’ 0.38 ms
å¼€å‘: 3 å¤©
ROI = (0.12 ms Ã— 1M) / 1000 / 3600 / 3 = 0.011 å°æ—¶/å¤©
ç»“è®º: âŒ æµªè´¹æ—¶é—´ï¼Œè½¬å‘ç³»ç»Ÿçº§ä¼˜åŒ–
```

---

## äº”ã€åé¢æ•™æ (é¿å…æµªè´¹æ—¶é—´)

### 5.1 é”™è¯¯ä½¿ç”¨ Shared Memory

**æ¡ˆä¾‹**: Vector Add ä½¿ç”¨ Shared Memory

```c
// âŒ é”™è¯¯: æ— æ•°æ®é‡ç”¨
__shared__ float s_a[256];
__shared__ float s_b[256];

s_a[tid] = a[gid];  // ä» global è¯»åˆ° shared
s_b[tid] = b[gid];
__syncthreads();

c[gid] = s_a[tid] + s_b[tid];  // ä» shared è¯» (åªç”¨ 1 æ¬¡!)
```

**é—®é¢˜**:
- æ¯ä¸ªæ•°æ®åªç”¨ 1 æ¬¡ï¼Œæ— é‡ç”¨
- å¼•å…¥é¢å¤– `__syncthreads()` å¼€é”€
- æ€§èƒ½åè€Œä¸‹é™ 50%

**æ•™è®­**: Shared Memory åªåœ¨æœ‰æ•°æ®é‡ç”¨æ—¶ä½¿ç”¨

### 5.2 è¿‡åº¦ä¼˜åŒ–

**æ¡ˆä¾‹**: ç»§ç»­ä¼˜åŒ–å·²è¾¾ 90% æ•ˆç‡çš„ kernel

```
v4: 0.54 ms, æ•ˆç‡ 91%
é¢„æœŸ v5: 0.45 ms (1.2x)
å¼€å‘æ—¶é—´: 3 å¤©

ROI è®¡ç®—:
æ¯å¤©èŠ‚çœ = 0.09 ms Ã— 1M = 90 ç§’ = 0.025 å°æ—¶
ROI = 0.025 / 3 = 0.008 å°æ—¶/å¤©

å¯¹æ¯”: ä½¿ç”¨ç®—å­èåˆ
é¢„æœŸ: 3x (èåˆ 3 ä¸ª kernel)
å¼€å‘æ—¶é—´: 1 å¤©
ROI = (1.08 ms Ã— 1M) / 1000 / 3600 / 1 = 0.3 å°æ—¶/å¤©

ç»“è®º: ç®—å­èåˆ ROI é«˜ 37 å€!
```

### 5.3 å¿½ç•¥åº“çš„åŠ›é‡

**æ¡ˆä¾‹**: æ‰‹å†™ GEMM vs cuBLAS

```
æ‰‹å†™ä¼˜åŒ– GEMM:
- å¼€å‘: 2 å‘¨
- æ€§èƒ½: 5000 GFLOPS (25% å³°å€¼)

cuBLAS:
- å¼€å‘: 5 åˆ†é’Ÿ
- æ€§èƒ½: 15000 GFLOPS (77% å³°å€¼)

ç»“è®º: 3x æ€§èƒ½å·®è·ï¼Œ400 å€å¼€å‘æ—¶é—´å·®è·
```

**æ•™è®­**:
- çŸ©é˜µä¹˜æ³• â†’ cuBLAS
- å·ç§¯ â†’ cuDNN
- æ‰‹å†™åªç”¨äºå­¦ä¹ æˆ–ç‰¹æ®Šéœ€æ±‚

---

## å…­ã€å®æˆ˜æ¡ˆä¾‹

### 6.1 Vector Add (Memory-bound ä¼˜åŒ–)

**åˆå§‹é—®é¢˜**:
```
NCU è¯Šæ–­:
- Memory: 89%, SM: 12% â†’ Memory-bound
- sectors_per_request: 32.0 â†’ éåˆå¹¶è®¿é—®
```

**æ ¸å¿ƒä¼˜åŒ–**: ä¿®å¤ Coalescing
```c
// ä¿®æ”¹å‰
c[tid * 32] = a[tid * 32] + b[tid * 32];

// ä¿®æ”¹å
c[tid] = a[tid] + b[tid];
```

**ç»“æœ**:
- æ€§èƒ½: 12.5 ms â†’ 1.5 ms (8.3x)
- å¼€å‘æ—¶é—´: 10 åˆ†é’Ÿ
- sectors_per_request: 32.0 â†’ 1.0 âœ…

**åç»­ä¼˜åŒ–** (è¾¹é™…é€’å‡):
- float4 vectorization: 1.5 ms â†’ 0.7 ms (2.1x)
- Final tuning: 0.7 ms â†’ 0.54 ms (1.3x)

**åœæ­¢ä¿¡å·**: æ•ˆç‡ 91%, ç»§ç»­ä¼˜åŒ– ROI < 0.01

### 6.2 GEMM (Tiling ä¼˜åŒ–)

**åˆå§‹é—®é¢˜**:
```
NCU è¯Šæ–­:
- Memory: 92%, SM: 8% â†’ Memory-bound
- sectors_per_request: 1.2 â†’ è®¿é—®åŸºæœ¬åˆå¹¶
- dram__bytes: 8 GB (å®é™…æ•°æ®åªæœ‰ 8 MB!)
â†’ é‡å¤è¯»å–ä¸¥é‡
```

**æ ¸å¿ƒä¼˜åŒ–**: Shared Memory Tiling
```c
__shared__ float As[32][33];  // +1 padding
__shared__ float Bs[32][33];

for (int t = 0; t < K/32; t++) {
    // åŠ è½½ tile (æ¯ä¸ªå…ƒç´ åŠ è½½ 1 æ¬¡)
    As[ty][tx] = A[...];
    Bs[ty][tx] = B[...];
    __syncthreads();

    // è®¡ç®— (æ¯ä¸ªå…ƒç´ é‡ç”¨ 32 æ¬¡)
    for (int k = 0; k < 32; k++) {
        sum += As[ty][k] * Bs[k][tx];
    }
}
```

**ç»“æœ**:
- æ€§èƒ½: 150 GFLOPS â†’ 2500 GFLOPS (16.6x)
- å¼€å‘æ—¶é—´: 3 å°æ—¶
- dram__bytes: 8 GB â†’ 8 MB (1000x å‡å°‘) âœ…

**åœæ­¢ä¿¡å·**: æ•ˆç‡ 25%, ä½†ç»§ç»­æ‰‹å†™ä¼˜åŒ– ROI < 0.1ï¼Œåº”ä½¿ç”¨ cuBLAS (15000 GFLOPS)

---

## ä¸ƒã€å¿«é€Ÿå‚è€ƒ

### 7.1 NCU å…³é”®æŒ‡æ ‡é€ŸæŸ¥

| æŒ‡æ ‡ | ä½ç½® | æ­£å¸¸å€¼ | å¼‚å¸¸å€¼ â†’ ä¼˜åŒ– |
|------|------|--------|--------------|
| Memory Throughput | Speed of Light | < 60% | > 80% â†’ Memory-bound |
| SM Throughput | Speed of Light | - | < 20% ä¸” Memory > 80% |
| sectors_per_request | Memory Workload | < 1.5 | > 1.5 â†’ Coalescing |
| dram__bytes | Memory Workload | è¶Šå°‘è¶Šå¥½ | è¿œå¤§äºå®é™…æ•°æ® â†’ Tiling |
| bank_conflicts | L1/TEX Cache | ~0 | > 100 â†’ Padding |

### 7.2 ä¼˜åŒ–æŠ€æœ¯ä¼˜å…ˆçº§

| ä¼˜å…ˆçº§ | æŠ€æœ¯ | é€‚ç”¨åœºæ™¯ | é¢„æœŸæå‡ | å¼€å‘æ—¶é—´ |
|--------|------|----------|----------|----------|
| **P0** | Memory Coalescing | sectors > 1.5 | 5-10x | 30 åˆ†é’Ÿ |
| **P1** | Shared Memory | æœ‰æ•°æ®é‡ç”¨ | 10-20x | 3 å°æ—¶ |
| **P2** | Vectorization | å·²åˆå¹¶è®¿é—® | 1.5-2x | 1 å°æ—¶ |
| **P3** | Tensor Core | çŸ©é˜µä¹˜æ³• | 10-16x | 5 åˆ†é’Ÿ (cuBLAS) |
| P4 | Bank Conflict Fix | conflicts > 100 | 1.2-1.5x | 1 å°æ—¶ |
| P5 | Loop Unrolling | - | 1.1-1.2x | 30 åˆ†é’Ÿ |

### 7.3 å†³ç­–å¿«æ·æ–¹å¼

```bash
# 1. è¿è¡Œ NCU
ncu --set full --export my_kernel ./my_kernel

# 2. æ‰“å¼€ NCU-UI æŸ¥çœ‹ 2 ä¸ªæŒ‡æ ‡
# - Speed of Light: Memory% vs SM%
# - sectors_per_request

# 3. å¥—ç”¨å†³ç­–æ ‘
if Memory% > 80% and sectors > 1.5:
    ä¼˜åŒ– = "Memory Coalescing"
    é¢„æœŸ = "8x"
elif Memory% > 80% and sectors < 1.5:
    ä¼˜åŒ– = "Shared Memory (å¦‚æœæœ‰é‡ç”¨)"
    é¢„æœŸ = "15x"
elif SM% > 60%:
    ä¼˜åŒ– = "Tensor Core (cuBLAS)"
    é¢„æœŸ = "10x"
else:
    ä¼˜åŒ– = "å¢åŠ å¹¶è¡Œåº¦"

# 4. å®æ–½ä¼˜åŒ–

# 5. éªŒè¯
ncu --metrics sectors_per_request,dram__bytes ./my_kernel_v2
```

---

## å…«ã€å·¥å…·ä½¿ç”¨

### 8.1 è‡ªåŠ¨è¯Šæ–­å·¥å…·

```bash
# å¿«é€Ÿè¯Šæ–­
python tools/auto_profile.py ./my_kernel

è¾“å‡ºç¤ºä¾‹:
==============================
ç“¶é¢ˆ: Memory-bound
é—®é¢˜: éåˆå¹¶è®¿é—® (sectors=32.0)
ä¼˜åŒ–: Memory Coalescing
é¢„æœŸ: 8-10x
ROI: â­â­â­â­â­
==============================
```

### 8.2 å¯¹æ¯”å·¥å…·

```bash
# å¯¹æ¯”ä¼˜åŒ–å‰å
python tools/compare_versions.py ./v0 ./v1 ./v2

è¾“å‡º:
v0: 10.5 ms  (1.0x)
v1: 1.2 ms   (8.8x)  âœ… Coalescing
v2: 0.6 ms   (17.5x) âœ… Vectorization
```

### 8.3 å¯è§†åŒ–

```bash
python tools/visualize.py results.json --output=report.png
# ç”ŸæˆåŠ é€Ÿæ¯”å›¾è¡¨ã€NCU æŒ‡æ ‡é›·è¾¾å›¾ç­‰
```

---

## ä¹ã€æ ¸å¿ƒåŸåˆ™æ€»ç»“

1. **å¿«é€Ÿè¯Šæ–­**: 5 åˆ†é’Ÿçœ‹ 2 ä¸ªæŒ‡æ ‡ï¼Œä¸è¦é™·å…¥ç»†èŠ‚
2. **ç²¾å‡†æ‰“å‡»**: ä¼˜å…ˆä¿®å¤æœ€å¤§ç“¶é¢ˆ (é€šå¸¸æ˜¯ Coalescing æˆ– Tiling)
3. **åŠæ—¶åœæ‰‹**: æ•ˆç‡ > 80% åè½¬å‘ç³»ç»Ÿçº§ä¼˜åŒ–
4. **ROI é©±åŠ¨**: æ¯ä¸ªä¼˜åŒ–éƒ½è®¡ç®— ROIï¼Œä½äº 0.5 ä¸åš
5. **å–„ç”¨åº“**: æ ‡å‡†æ“ä½œ (GEMM, Conv) ç”¨ cuBLAS/cuDNN

**è®°ä½**:
> å®Œç¾æ˜¯ä¼˜ç§€çš„æ•Œäººã€‚80% çš„ä¼˜åŒ–æ•ˆæœ + 20% çš„æ—¶é—´ = æœ€ä½³ç­–ç•¥

---

## åã€å­¦ä¹ è·¯å¾„

**Day 1** (2 å°æ—¶):
1. è¯»æœ¬æ–‡æ¡£ (30 åˆ†é’Ÿ)
2. è¿è¡Œ Vector Add benchmark (30 åˆ†é’Ÿ)
3. æŸ¥çœ‹ NCU æŠ¥å‘Š (1 å°æ—¶)

**Day 2** (3 å°æ—¶):
1. è¯» NCU_STEP_BY_STEP.md (1 å°æ—¶)
2. è‡ªå·±ä¿®æ”¹ä¸€ä¸ª naive kernel (2 å°æ—¶)

**Day 3** (4 å°æ—¶):
1. å­¦ä¹  GEMM Shared Memory Tiling (2 å°æ—¶)
2. å®è·µè‡ªå·±çš„é¡¹ç›® (2 å°æ—¶)

**æŒç»­**:
- é‡åˆ°é—®é¢˜æŸ¥ `techniques/` å¯¹åº”ç« èŠ‚
- ç”¨ `tools/` è‡ªåŠ¨åŒ–åˆ†æ
- è®°ä½ï¼šå¿«é€Ÿè¿­ä»£ > å®Œç¾ä¼˜åŒ–
