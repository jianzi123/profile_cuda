# GPU ä¼˜åŒ–æŠ€æœ¯å†³ç­–å®Œå…¨æŒ‡å—

## ç›®å½•
1. [ä¼˜åŒ–å†³ç­–æ€»è§ˆ](#ä¼˜åŒ–å†³ç­–æ€»è§ˆ)
2. [ç®—å­èåˆ - ä½•æ—¶ä½¿ç”¨](#ç®—å­èåˆ---ä½•æ—¶ä½¿ç”¨)
3. [ILP ä¼˜åŒ– - ä½•æ—¶ä½¿ç”¨](#ilp-ä¼˜åŒ–---ä½•æ—¶ä½¿ç”¨)
4. [å‘é‡åŒ– - ä½•æ—¶ä½¿ç”¨](#å‘é‡åŒ–---ä½•æ—¶ä½¿ç”¨)
5. [Shared Memory - ä½•æ—¶ä½¿ç”¨](#shared-memory---ä½•æ—¶ä½¿ç”¨)
6. [å¾ªç¯å±•å¼€ - ä½•æ—¶ä½¿ç”¨](#å¾ªç¯å±•å¼€---ä½•æ—¶ä½¿ç”¨)
7. [Tensor Cores - ä½•æ—¶ä½¿ç”¨](#tensor-cores---ä½•æ—¶ä½¿ç”¨)
8. [å®Œæ•´å†³ç­–æµç¨‹](#å®Œæ•´å†³ç­–æµç¨‹)
9. [å®æˆ˜æ¡ˆä¾‹](#å®æˆ˜æ¡ˆä¾‹)

---

## ä¼˜åŒ–å†³ç­–æ€»è§ˆ

### ä¼˜åŒ–æŠ€æœ¯åˆ†ç±»

```
GPU ä¼˜åŒ–æŠ€æœ¯
â”‚
â”œâ”€ 1. ç³»ç»Ÿçº§ä¼˜åŒ–ï¼ˆå½±å“æœ€å¤§ï¼Œä¼˜å…ˆçº§æœ€é«˜ï¼‰
â”‚  â”œâ”€ ç®—å­èåˆ (Kernel Fusion)          - 3-10x æå‡
â”‚  â”œâ”€ CUDA Graphs                       - 2-3x æå‡ï¼ˆå°kernelï¼‰
â”‚  â””â”€ å¤šæµå¹¶è¡Œ (Multi-Stream)           - 1.5-2x æå‡
â”‚
â”œâ”€ 2. å†…å­˜ä¼˜åŒ–ï¼ˆMemory-bound å¿…åšï¼‰
â”‚  â”œâ”€ Shared Memory ç¼“å­˜                - 2-5x æå‡
â”‚  â”œâ”€ å‘é‡åŒ–è®¿é—® (float4)               - 1.5-2x æå‡
â”‚  â”œâ”€ Coalesced Access                  - 2-4x æå‡
â”‚  â””â”€ Bank Conflict æ¶ˆé™¤                - 1.2-1.5x æå‡
â”‚
â”œâ”€ 3. è®¡ç®—ä¼˜åŒ–ï¼ˆCompute-bound å¿…åšï¼‰
â”‚  â”œâ”€ Tensor Cores                      - 5-20x æå‡
â”‚  â”œâ”€ ILP æå‡                          - 1.3-2x æå‡
â”‚  â”œâ”€ Warp Divergence æ¶ˆé™¤              - 1.5-3x æå‡
â”‚  â””â”€ å¿«é€Ÿæ•°å­¦å‡½æ•°                      - 1.5-3x æå‡ï¼ˆè¶…è¶Šå‡½æ•°ï¼‰
â”‚
â””â”€ 4. å¾®è°ƒä¼˜åŒ–ï¼ˆé”¦ä¸Šæ·»èŠ±ï¼‰
   â”œâ”€ å¾ªç¯å±•å¼€                          - 1.1-1.3x æå‡
   â”œâ”€ Occupancy è°ƒä¼˜                    - 1.1-1.5x æå‡
   â””â”€ å¯„å­˜å™¨ä¼˜åŒ–                        - 1.05-1.2x æå‡
```

### å†³ç­–ä¼˜å…ˆçº§

```
ç¬¬ä¸€ä¼˜å…ˆçº§ï¼šç³»ç»Ÿçº§ä¼˜åŒ–
  â†’ å¦‚æœæœ‰å¤šä¸ªå° kernel â†’ ç®—å­èåˆ
  â†’ å¦‚æœ kernel åºåˆ—é‡å¤æ‰§è¡Œ â†’ CUDA Graphs

ç¬¬äºŒä¼˜å…ˆçº§ï¼šæ ¹æ®ç“¶é¢ˆç±»å‹
  â†’ Memory-bound â†’ å†…å­˜ä¼˜åŒ–
  â†’ Compute-bound â†’ è®¡ç®—ä¼˜åŒ–

ç¬¬ä¸‰ä¼˜å…ˆçº§ï¼šç»†èŠ‚ä¼˜åŒ–
  â†’ åœ¨å‰ä¸¤æ­¥åŸºç¡€ä¸Šè¿›è¡Œå¾®è°ƒ
```

---

## ç®—å­èåˆ - ä½•æ—¶ä½¿ç”¨

### ğŸ“‹ åˆ¤æ–­æ ‡å‡†

#### âœ… åº”è¯¥ä½¿ç”¨ç®—å­èåˆçš„æƒ…å†µ

**1. å­˜åœ¨å¤šä¸ªè¿ç»­çš„ Element-wise æ“ä½œ**

```python
# âŒ èåˆå‰ï¼š3 ä¸ª kernel
x = relu(x)           # Kernel 1: 4 ms
x = x + bias         # Kernel 2: 4 ms
x = x * scale        # Kernel 3: 4 ms
# æ€»æ—¶é—´ï¼š12 ms + kernel å¯åŠ¨å¼€é”€

# âœ… èåˆåï¼š1 ä¸ª kernel
x = fused_relu_bias_scale(x, bias, scale)
# æ€»æ—¶é—´ï¼š4 ms
# æå‡ï¼š3x
```

**NCU ç‰¹å¾**ï¼š
```bash
ncu --section SpeedOfLight

çœ‹åˆ°ï¼š
- SM Throughput: 10-20%ï¼ˆè®¡ç®—å•å…ƒç©ºé—²ï¼‰
- Memory Throughput: 80-95%ï¼ˆå†…å­˜é¥±å’Œï¼‰
- Duration: å¾ˆçŸ­ï¼ˆ< 10 msï¼‰

åŒæ—¶ nsys æ˜¾ç¤ºï¼š
- å¤šä¸ªå° kernel è¿ç»­æ‰§è¡Œ
- Kernel ä¹‹é—´æœ‰ gap
```

**å…·ä½“åˆ¤æ–­æ¡ä»¶**ï¼š
```
æ»¡è¶³ä»¥ä¸‹ä»»ä¸€æ¡ä»¶å°±åº”è¯¥èåˆï¼š

1. æœ‰ 3 ä¸ªä»¥ä¸Šè¿ç»­çš„ element-wise kernel
   â†’ èåˆé¢„æœŸæå‡ï¼š2-5x

2. kernel duration < 100 Î¼s
   â†’ kernel å¯åŠ¨å¼€é”€å æ¯” > 10%
   â†’ èåˆé¢„æœŸæå‡ï¼š2-3x

3. L2 hit rate < 30% ä¸”æ˜¯è¿ç»­ kernel
   â†’ ä¸­é—´ç»“æœæ²¡æœ‰å¤ç”¨
   â†’ èåˆé¢„æœŸæå‡ï¼š3-10x

4. ç®—å­ä¹‹é—´æœ‰ç›¸åŒçš„è¾“å…¥
   â†’ å¯ä»¥å…±äº«è¯»å–
   â†’ èåˆé¢„æœŸæå‡ï¼š1.5-3x
```

**2. æœ‰ä¸­é—´ç»“æœå¯ä»¥æ¶ˆé™¤**

```python
# âŒ èåˆå‰
temp1 = matmul(A, B)      # å†™ temp1 åˆ° global memory
temp2 = relu(temp1)       # è¯» temp1ï¼Œå†™ temp2
output = add(temp2, bias) # è¯» temp2

# âœ… èåˆå
output = fused_matmul_relu_bias(A, B, bias)
# temp1, temp2 åªå­˜åœ¨äº register/shared memory
```

**NCU ç‰¹å¾**ï¼š
```
ç¬¬ä¸€ä¸ª kernel çš„ dram__bytes_write.sum
â‰ˆ ç¬¬äºŒä¸ª kernel çš„ dram__bytes_read.sum

è¯´æ˜ï¼šç¬¬ä¸€ä¸ª kernel çš„è¾“å‡ºæ˜¯ç¬¬äºŒä¸ªçš„è¾“å…¥
â†’ å¯ä»¥èåˆï¼Œæ¶ˆé™¤ä¸­é—´å†…å­˜ä¼ è¾“
```

**3. å° kernel å¯†é›†æ‰§è¡Œ**

```bash
# nsys æ—¶é—´çº¿æ˜¾ç¤º
Kernel1 | Kernel2 | Kernel3 | Kernel4 | ...
   1ms     1ms       1ms       1ms
```

**åˆ¤æ–­æ¡ä»¶**ï¼š
```
å¦‚æœï¼š
- å•ä¸ª kernel duration < 5 ms
- è¿ç»­æ‰§è¡Œ > 5 ä¸ª kernel
- GPU utilization < 60%ï¼ˆæœ‰ gapï¼‰

â†’ åº”è¯¥èåˆ
â†’ é¢„æœŸæå‡ï¼š2-5x
```

#### âŒ ä¸åº”è¯¥ä½¿ç”¨ç®—å­èåˆçš„æƒ…å†µ

**1. Kernel ä¹‹é—´æœ‰æ•°æ®ä¾èµ–**

```cpp
// âŒ æ— æ³•èåˆ
x = compute1(data);
y = compute2(x);    // ä¾èµ– x çš„å…¨éƒ¨ç»“æœ
z = compute3(x, y); // ä¾èµ– x å’Œ y çš„å…¨éƒ¨ç»“æœ

// åŸå› ï¼šcompute2 éœ€è¦ç­‰å¾… compute1 å®Œå…¨ç»“æŸ
```

**2. Kernel å·²ç»å¾ˆå¤§ä¸” compute-bound**

```
å¦‚æœå•ä¸ª kernelï¼š
- Duration > 50 ms
- SM Throughput > 80%
- Compute-bound

â†’ ä¸è¦èåˆï¼ˆå·²ç»å¾ˆé«˜æ•ˆï¼‰
â†’ èåˆå¯èƒ½å¯¼è‡´å¯„å­˜å™¨æº¢å‡ºï¼Œé™ä½æ€§èƒ½
```

**3. èåˆåå¯„å­˜å™¨/Shared Memory æº¢å‡º**

```cpp
// æ£€æŸ¥èµ„æºä½¿ç”¨
nvcc --ptxas-options=-v kernel.cu

è¾“å‡ºï¼š
ptxas info    : Used 128 registers per thread
ptxas info    : Used 96 KB shared memory

å¦‚æœèåˆåï¼š
ptxas info    : Used 255 registers per thread  // æ¥è¿‘ä¸Šé™ï¼
ptxas info    : Used 164 KB shared memory      // è¶…è¿‡é™åˆ¶ï¼

â†’ ä¸è¦èåˆ
â†’ ä¼šå¯¼è‡´å ç”¨ç‡å¤§å¹…ä¸‹é™
```

### ğŸ“Š èåˆæ•ˆæœé¢„æµ‹

| åœºæ™¯ | èåˆå‰ | èåˆå | æå‡ |
|------|--------|--------|------|
| 3ä¸ª element-wise | 12 ms | 4 ms | 3x |
| 5ä¸ª element-wise | 20 ms | 4 ms | 5x |
| MatMul + ReLU + Bias | 15 ms | 12 ms | 1.25x |
| Conv + BN + ReLU | 20 ms | 15 ms | 1.33x |
| Softmaxï¼ˆå†…éƒ¨èåˆï¼‰ | 8 ms | 2 ms | 4x |

### ğŸ’¡ èåˆå®è·µæŒ‡å—

```cpp
// æ­¥éª¤ 1ï¼šè¯†åˆ«å¯èåˆçš„ kernel
// ä½¿ç”¨ nsys æŸ¥çœ‹æ—¶é—´çº¿
nsys profile -o timeline ./program
// æ‰¾åˆ°è¿ç»­çš„å° kernel

// æ­¥éª¤ 2ï¼šç¼–å†™èåˆ kernel
__global__ void fused_kernel(
    float* data,
    float bias,
    float scale,
    int N
) {
    int idx = blockIdx.x * blockDim.x + threadIdx.x;
    if (idx < N) {
        // èåˆå¤šä¸ªæ“ä½œ
        float val = data[idx];
        val = fmaxf(0.0f, val);  // ReLU
        val += bias;              // Add
        val *= scale;             // Scale
        data[idx] = val;
    }
}

// æ­¥éª¤ 3ï¼šéªŒè¯æ€§èƒ½
// Before:
ncu --section SpeedOfLight ./program_old
// After:
ncu --section SpeedOfLight ./program_fused

// æ­¥éª¤ 4ï¼šæ£€æŸ¥æ­£ç¡®æ€§
// å¯¹æ¯”ç»“æœ
```

---

## ILP ä¼˜åŒ– - ä½•æ—¶ä½¿ç”¨

### ğŸ“‹ åˆ¤æ–­æ ‡å‡†

#### âœ… åº”è¯¥ä½¿ç”¨ ILP ä¼˜åŒ–çš„æƒ…å†µ

**1. Compute-bound ä½†æŒ‡ä»¤å‘å°„å—é™**

**NCU ç‰¹å¾**ï¼š
```bash
ncu --section ComputeWorkloadAnalysis

çœ‹åˆ°ï¼š
SM Throughput: > 80%ï¼ˆè®¡ç®—ä»»åŠ¡é‡ï¼‰
ä½†æ˜¯ï¼š
smsp__issue_active: < 60%ï¼ˆæŒ‡ä»¤å‘å°„ç‡ä½ï¼‰
smsp__inst_executed_pipe_alu: < 70%ï¼ˆALU æµæ°´çº¿åˆ©ç”¨ç‡ä½ï¼‰

åŸå› ï¼šæŒ‡ä»¤é—´ä¾èµ–å¤ªå¼ºï¼Œæ— æ³•å¹¶è¡Œå‘å°„
è§£å†³ï¼šæé«˜ ILP
```

**åˆ¤æ–­å…¬å¼**ï¼š
```python
if (SM_Throughput > 80 and issue_active < 60):
    print("éœ€è¦ ILP ä¼˜åŒ–")
    print("é¢„æœŸæå‡ï¼š1.3-2x")
```

**2. æ¯ä¸ªçº¿ç¨‹çš„ç‹¬ç«‹æ“ä½œå°‘**

```cpp
// âŒ ILP ä½
__global__ void low_ilp(float* data, int N) {
    int idx = blockIdx.x * blockDim.x + threadIdx.x;
    if (idx < N) {
        float a = data[idx];
        a = a * 2.0f;    // ä¾èµ–å‰ä¸€æ¡
        a = a + 1.0f;    // ä¾èµ–å‰ä¸€æ¡
        a = a * a;       // ä¾èµ–å‰ä¸€æ¡
        data[idx] = a;   // ä¾èµ–å‰ä¸€æ¡
        // æ¯æ¡æŒ‡ä»¤éƒ½ä¾èµ–å‰ä¸€æ¡ï¼Œæ— æ³•å¹¶è¡Œ
    }
}

NCU æ˜¾ç¤ºï¼š
smsp__issue_active: 40%
```

**3. æœ‰å¤§é‡ç‹¬ç«‹çš„æ•°æ®å¯ä»¥å¤„ç†**

```cpp
// âœ… ILP é«˜
__global__ void high_ilp(float* data, int N) {
    int idx = (blockIdx.x * blockDim.x + threadIdx.x) * 4;
    if (idx + 3 < N) {
        // 4 ä¸ªç‹¬ç«‹çš„æ“ä½œå¯ä»¥å¹¶è¡Œ
        float a0 = data[idx + 0];
        float a1 = data[idx + 1];
        float a2 = data[idx + 2];
        float a3 = data[idx + 3];

        // è¿™äº›æ“ä½œæ˜¯ç‹¬ç«‹çš„ï¼Œå¯ä»¥å¹¶è¡Œæ‰§è¡Œ
        a0 = a0 * 2.0f + 1.0f;
        a1 = a1 * 2.0f + 1.0f;
        a2 = a2 * 2.0f + 1.0f;
        a3 = a3 * 2.0f + 1.0f;

        data[idx + 0] = a0;
        data[idx + 1] = a1;
        data[idx + 2] = a2;
        data[idx + 3] = a3;
    }
}

NCU æ˜¾ç¤ºï¼š
smsp__issue_active: 75%ï¼ˆæå‡ï¼ï¼‰
```

#### âŒ ä¸åº”è¯¥ä½¿ç”¨ ILP ä¼˜åŒ–çš„æƒ…å†µ

**1. Memory-bound kernel**

```
å¦‚æœï¼š
Memory Throughput > 80%
SM Throughput < 40%

â†’ ä¸è¦åš ILP ä¼˜åŒ–
â†’ ç“¶é¢ˆåœ¨å†…å­˜ï¼Œä¸åœ¨è®¡ç®—
â†’ åº”è¯¥å…ˆåšå†…å­˜ä¼˜åŒ–
```

**2. å·²ç»æœ‰å¾ˆå¥½çš„ ILP**

```
å¦‚æœï¼š
smsp__issue_active > 75%

â†’ å·²ç»å¾ˆå¥½äº†
â†’ ILP ä¼˜åŒ–ç©ºé—´å°
```

**3. å¯„å­˜å™¨å‹åŠ›å¤§**

```cpp
// æ£€æŸ¥å¯„å­˜å™¨ä½¿ç”¨
nvcc --ptxas-options=-v kernel.cu

è¾“å‡ºï¼š
ptxas info : Used 200 registers per thread

å¦‚æœæé«˜ ILPï¼ˆæ¯ä¸ªçº¿ç¨‹å¤„ç†æ›´å¤šæ•°æ®ï¼‰ï¼š
ptxas info : Used 255 registers per thread  // æ¥è¿‘ä¸Šé™

â†’ ä¼šå¯¼è‡´ occupancy é™ä½
â†’ å¾—ä¸å¿å¤±
```

### ğŸ“Š ILP ä¼˜åŒ–æ•ˆæœ

| åœºæ™¯ | ILP å‰ issue_active | ILP å issue_active | æå‡ |
|------|-------------------|-------------------|------|
| Element-wise (ç®€å•) | 45% | 72% | 1.6x |
| Element-wise (å¤æ‚) | 38% | 68% | 1.8x |
| Reduction | 52% | 75% | 1.4x |
| å·²ä¼˜åŒ– MatMul | 78% | 82% | 1.05xï¼ˆç©ºé—´å°ï¼‰ |

### ğŸ’¡ ILP å®è·µæŒ‡å—

```cpp
// æ­¥éª¤ 1ï¼šç¡®è®¤éœ€è¦ ILP ä¼˜åŒ–
ncu --section ComputeWorkloadAnalysis ./program

// æ­¥éª¤ 2ï¼šä¿®æ”¹ä»£ç 
// Before
__global__ void kernel_v1(float* data, int N) {
    int idx = blockIdx.x * blockDim.x + threadIdx.x;
    if (idx < N) {
        float val = compute(data[idx]);
        data[idx] = val;
    }
}

// After - æ¯ä¸ªçº¿ç¨‹å¤„ç† 4 ä¸ªå…ƒç´ 
__global__ void kernel_v2_ilp(float* data, int N) {
    int base = (blockIdx.x * blockDim.x + threadIdx.x) * 4;

    if (base + 3 < N) {
        // ç‹¬ç«‹çš„ 4 ä¸ªè®¡ç®—
        float v0 = compute(data[base + 0]);
        float v1 = compute(data[base + 1]);
        float v2 = compute(data[base + 2]);
        float v3 = compute(data[base + 3]);

        data[base + 0] = v0;
        data[base + 1] = v1;
        data[base + 2] = v2;
        data[base + 3] = v3;
    }
}

// è°ƒæ•´å¯åŠ¨é…ç½®
int threads = 256;
int blocks_v1 = (N + threads - 1) / threads;
int blocks_v2 = ((N/4) + threads - 1) / threads;  // æ³¨æ„é™¤ä»¥ 4

// æ­¥éª¤ 3ï¼šéªŒè¯
ncu --metrics smsp__issue_active ./program_v2
```

---

## å‘é‡åŒ– - ä½•æ—¶ä½¿ç”¨

### ğŸ“‹ åˆ¤æ–­æ ‡å‡†

#### âœ… åº”è¯¥ä½¿ç”¨å‘é‡åŒ–çš„æƒ…å†µ

**1. Memory-bound ä¸”è®¿é—®å·²ç» coalesced**

**NCU ç‰¹å¾**ï¼š
```bash
ncu --section MemoryWorkloadAnalysis

çœ‹åˆ°ï¼š
Memory Throughput: > 80%
l1tex__average_t_sectors_per_request: â‰ˆ 1.0ï¼ˆå·²ç» coalescedï¼‰

ä½†æ˜¯ï¼š
å®é™…å¸¦å®½ < 80% ç†è®ºå¸¦å®½

â†’ å¯ä»¥é€šè¿‡å‘é‡åŒ–è¿›ä¸€æ­¥æå‡å¸¦å®½åˆ©ç”¨ç‡
```

**2. Element-wise æ“ä½œ**

```cpp
// âœ… é€‚åˆå‘é‡åŒ–
// æ¯ä¸ªå…ƒç´ çš„è®¡ç®—æ˜¯ç‹¬ç«‹çš„
C[i] = A[i] + B[i]
C[i] = relu(A[i])
C[i] = A[i] * scale + bias

// âŒ ä¸é€‚åˆå‘é‡åŒ–
// éœ€è¦è§„çº¦æ“ä½œ
sum += A[i]
```

**3. æ•°æ®å¯¹é½**

```cpp
// æ£€æŸ¥æ•°æ®å¯¹é½
void* ptr;
cudaMalloc(&ptr, N * sizeof(float));

// cudaMalloc ä¿è¯ 256-byte å¯¹é½
// å¯ä»¥å®‰å…¨ä½¿ç”¨ float4

// å¦‚æœä½¿ç”¨ cudaMallocPitch
size_t pitch;
cudaMallocPitch(&ptr, &pitch, width * sizeof(float), height);
// pitch ä¹Ÿæ˜¯å¯¹é½çš„
```

**åˆ¤æ–­æ¡ä»¶**ï¼š
```
å¦‚æœæ»¡è¶³ï¼š
1. Memory-bound
2. Element-wise æ“ä½œ
3. æ•°æ®å¯¹é½
4. N % 4 == 0ï¼ˆæˆ–å¯ä»¥å¤„ç†è¾¹ç•Œï¼‰

â†’ åº”è¯¥ä½¿ç”¨ float4 å‘é‡åŒ–
â†’ é¢„æœŸæå‡ï¼š1.5-2x
```

#### âŒ ä¸åº”è¯¥ä½¿ç”¨å‘é‡åŒ–çš„æƒ…å†µ

**1. Compute-bound kernel**

```
å¦‚æœï¼š
SM Throughput > 80%
Memory Throughput < 60%

â†’ å‘é‡åŒ–æ²¡ç”¨ï¼ˆç“¶é¢ˆä¸åœ¨å†…å­˜ï¼‰
â†’ åº”è¯¥åšè®¡ç®—ä¼˜åŒ–
```

**2. æ•°æ®æœªå¯¹é½**

```cpp
// âŒ æ•°æ®æœªå¯¹é½
float* data = some_pointer + 1;  // åç§» 1 ä¸ªå…ƒç´ ï¼Œæœªå¯¹é½

// ä½¿ç”¨ float4 ä¼šå¯¼è‡´ unaligned access
float4 val = reinterpret_cast<float4*>(data)[idx];  // å¯èƒ½å´©æºƒæˆ–å¾ˆæ…¢
```

**3. å¤æ‚çš„æ•°æ®ä¾èµ–**

```cpp
// âŒ ä¸é€‚åˆå‘é‡åŒ–
__global__ void prefix_sum(float* data, int N) {
    int idx = blockIdx.x * blockDim.x + threadIdx.x;
    if (idx > 0 && idx < N) {
        data[idx] += data[idx - 1];  // ä¾èµ–å‰ä¸€ä¸ªå…ƒç´ 
    }
}
// æ— æ³•ç®€å•å‘é‡åŒ–
```

### ğŸ“Š å‘é‡åŒ–æ•ˆæœ

| åœºæ™¯ | æ ‡é‡ç‰ˆæœ¬ | float4 ç‰ˆæœ¬ | æå‡ |
|------|---------|-----------|------|
| Vector Add | 800 GB/s | 1200 GB/s | 1.5x |
| ReLU | 750 GB/s | 1150 GB/s | 1.53x |
| GELU | 650 GB/s | 1050 GB/s | 1.6x |
| Element-wise èåˆ | 900 GB/s | 1400 GB/s | 1.56x |

### ğŸ’¡ å‘é‡åŒ–å®è·µæŒ‡å—

```cpp
// æ­¥éª¤ 1ï¼šæ£€æŸ¥æ˜¯å¦é€‚åˆ
ncu --section MemoryWorkloadAnalysis ./program
// ç¡®è®¤ Memory-bound ä¸” coalesced

// æ­¥éª¤ 2ï¼šå®ç°å‘é‡åŒ–
// Before
__global__ void kernel_scalar(float* out, const float* in, int N) {
    int idx = blockIdx.x * blockDim.x + threadIdx.x;
    if (idx < N) {
        out[idx] = in[idx] * 2.0f + 1.0f;
    }
}

// After
__global__ void kernel_vectorized(float* out, const float* in, int N) {
    int idx = blockIdx.x * blockDim.x + threadIdx.x;
    int vec_idx = idx * 4;

    if (vec_idx + 3 < N) {
        float4 val = reinterpret_cast<const float4*>(in)[idx];

        val.x = val.x * 2.0f + 1.0f;
        val.y = val.y * 2.0f + 1.0f;
        val.z = val.z * 2.0f + 1.0f;
        val.w = val.w * 2.0f + 1.0f;

        reinterpret_cast<float4*>(out)[idx] = val;
    }

    // å¤„ç†å‰©ä½™å…ƒç´ 
    for (int i = (N/4)*4; i < N; i++) {
        if (blockIdx.x * blockDim.x + threadIdx.x == 0 &&
            threadIdx.x < (N % 4)) {
            out[i] = in[i] * 2.0f + 1.0f;
        }
    }
}

// è°ƒæ•´å¯åŠ¨é…ç½®
int vec_threads = N / 4;
int blocks = (vec_threads + 255) / 256;
kernel_vectorized<<<blocks, 256>>>(out, in, N);

// æ­¥éª¤ 3ï¼šéªŒè¯
// æ£€æŸ¥å¸¦å®½æå‡
ncu --metrics dram__throughput ./program_vectorized
```

---

## Shared Memory - ä½•æ—¶ä½¿ç”¨

### ğŸ“‹ åˆ¤æ–­æ ‡å‡†

#### âœ… åº”è¯¥ä½¿ç”¨ Shared Memory çš„æƒ…å†µ

**1. æ•°æ®æœ‰å¤ç”¨ï¼ˆåŒä¸€æ•°æ®è¢«å¤šæ¬¡è®¿é—®ï¼‰**

**NCU ç‰¹å¾**ï¼š
```bash
ncu --section MemoryWorkloadAnalysis

çœ‹åˆ°ï¼š
L2 hit rate < 50%ï¼ˆæ•°æ®æ²¡æœ‰åœ¨ L2 ä¸­å¤ç”¨ï¼‰
ä½†æ˜¯ï¼š
ç®—æ³•ä¸ŠåŒä¸€æ•°æ®ä¼šè¢«å¤šæ¬¡è®¿é—®

â†’ åº”è¯¥ç”¨ Shared Memory ç¼“å­˜
```

**å…¸å‹åœºæ™¯**ï¼š

**çŸ©é˜µä¹˜æ³• - Tiling**
```cpp
// C = A Ã— B
// æ¯ä¸ª A çš„å…ƒç´ è¢«è¯»å– N æ¬¡ï¼ˆB çš„åˆ—æ•°ï¼‰
// æ¯ä¸ª B çš„å…ƒç´ è¢«è¯»å– M æ¬¡ï¼ˆA çš„è¡Œæ•°ï¼‰

// âœ… ä½¿ç”¨ Shared Memory
__global__ void matmul_tiled(float* C, const float* A, const float* B,
                              int M, int N, int K) {
    __shared__ float As[TILE_SIZE][TILE_SIZE];
    __shared__ float Bs[TILE_SIZE][TILE_SIZE];

    // åŠ è½½åˆ° shared memoryï¼ˆå¤ç”¨ TILE_SIZE æ¬¡ï¼‰
    As[ty][tx] = A[...];
    Bs[ty][tx] = B[...];
    __syncthreads();

    // ä½¿ç”¨ shared memory ä¸­çš„æ•°æ®ï¼ˆå¿«ï¼ï¼‰
    for (int k = 0; k < TILE_SIZE; k++) {
        sum += As[ty][k] * Bs[k][tx];
    }
}
```

**å·ç§¯**
```cpp
// æ¯ä¸ªè¾“å…¥å…ƒç´ è¢«å¤šä¸ªè¾“å‡ºå…ƒç´ ä½¿ç”¨ï¼ˆkernel size æ¬¡ï¼‰
// âœ… ä½¿ç”¨ Shared Memory ç¼“å­˜è¾“å…¥
```

**Stencil æ“ä½œ**
```cpp
// æ¯ä¸ªå…ƒç´ éœ€è¦è®¿é—®é‚»å±…å…ƒç´ 
// âœ… ä½¿ç”¨ Shared Memory ç¼“å­˜å—
```

**2. L1 cache miss ä¸¥é‡**

```bash
ncu --section MemoryWorkloadAnalysis

çœ‹åˆ°ï¼š
l1tex__t_sector_hit_rate.pct < 70%

åŸå› ï¼šL1 cache å¤ªå°ï¼Œæ— æ³•å®¹çº³æ‰€æœ‰æ•°æ®
è§£å†³ï¼šæ˜¾å¼ä½¿ç”¨ Shared Memory
```

**3. éœ€è¦çº¿ç¨‹é—´é€šä¿¡**

```cpp
// Block å†…çº¿ç¨‹éœ€è¦äº¤æ¢æ•°æ®
// âœ… å¿…é¡»ä½¿ç”¨ Shared Memory

// ä¾‹å¦‚ï¼šTranspose
__global__ void transpose(float* out, const float* in) {
    __shared__ float tile[32][33];  // +1 é¿å… bank conflict

    // çº¿ç¨‹åä½œåŠ è½½
    tile[ty][tx] = in[...];
    __syncthreads();

    // è½¬ç½®åçš„è®¿é—®æ¨¡å¼
    out[...] = tile[tx][ty];
}
```

#### âŒ ä¸åº”è¯¥ä½¿ç”¨ Shared Memory çš„æƒ…å†µ

**1. æ•°æ®æ²¡æœ‰å¤ç”¨**

```cpp
// âŒ ä¸éœ€è¦ Shared Memory
__global__ void vector_add(float* C, const float* A, const float* B) {
    int idx = blockIdx.x * blockDim.x + threadIdx.x;
    C[idx] = A[idx] + B[idx];
    // æ¯ä¸ªæ•°æ®åªè®¿é—®ä¸€æ¬¡ï¼Œä¸éœ€è¦ç¼“å­˜
}
```

**2. Shared Memory ä¼šå¯¼è‡´ Bank Conflicts**

```bash
ncu --section MemoryWorkloadAnalysis

å¦‚æœä½¿ç”¨ Shared Memory åï¼š
l1tex__data_bank_conflicts_pipe_lsu.sum > 1000

ä¸”ï¼š
conflict_rate > 20%

â†’ Shared Memory åè€Œé™ä½æ€§èƒ½
â†’ éœ€è¦é‡æ–°è®¾è®¡è®¿é—®æ¨¡å¼æˆ–ä¸ç”¨ Shared Memory
```

**3. Shared Memory é™åˆ¶ Occupancy**

```bash
ncu --section Occupancy

çœ‹åˆ°ï¼š
Limiting Factor: Shared Memory
Theoretical Occupancy: 25%ï¼ˆå› ä¸º Shared Memory å¤ªå¤šï¼‰

â†’ å‡å°‘ Shared Memory ä½¿ç”¨æˆ–ä¸ç”¨
```

### ğŸ“Š Shared Memory æ•ˆæœ

| åœºæ™¯ | ä¸ç”¨ Shared Memory | ç”¨ Shared Memory | æå‡ |
|------|------------------|----------------|------|
| MatMul (Naive â†’ Tiled) | 500 GFLOPS | 2000 GFLOPS | 4x |
| Convolution | 1.2 TFLOPS | 3.5 TFLOPS | 2.9x |
| Transpose | 200 GB/s | 800 GB/s | 4x |
| Reduction | 150 GB/s | 600 GB/s | 4x |
| Vector Add | 1000 GB/s | 1000 GB/s | 1xï¼ˆæ— ç”¨ï¼‰ |

### ğŸ’¡ Shared Memory å®è·µæŒ‡å—

```cpp
// æ­¥éª¤ 1ï¼šåˆ†ææ•°æ®å¤ç”¨
// åŒä¸€æ•°æ®è¢«è®¿é—®å‡ æ¬¡ï¼Ÿ
// å¦‚æœ > 1 æ¬¡ï¼Œè€ƒè™‘ Shared Memory

// æ­¥éª¤ 2ï¼šè®¡ç®— Shared Memory å¤§å°
// æ¯ä¸ª block éœ€è¦å¤šå°‘ï¼Ÿ
int shared_per_block = TILE_SIZE * TILE_SIZE * sizeof(float);
// A100 æ¯ä¸ª SM æœ‰ 164 KB

// æ­¥éª¤ 3ï¼šå®ç°
__global__ void kernel_with_shared(float* data, int N) {
    __shared__ float shared[TILE_SIZE][TILE_SIZE + 1];  // +1 é¿å… bank conflict

    // åŠ è½½åˆ° shared memory
    shared[ty][tx] = data[...];
    __syncthreads();  // ç­‰å¾…æ‰€æœ‰çº¿ç¨‹åŠ è½½å®Œæˆ

    // ä½¿ç”¨ shared memory
    float val = 0;
    for (int i = 0; i < TILE_SIZE; i++) {
        val += shared[ty][i] * something;
    }

    __syncthreads();  // å¦‚æœåç»­è¿˜è¦ç”¨ï¼Œéœ€è¦åŒæ­¥
}

// æ­¥éª¤ 4ï¼šéªŒè¯
ncu --section MemoryWorkloadAnalysis ./program

// æ£€æŸ¥ï¼š
// 1. DRAM è®¿é—®æ˜¯å¦å‡å°‘
// 2. Bank conflicts æ˜¯å¦ä¸º 0
// 3. Occupancy æ˜¯å¦å—å½±å“
```

---

## å¾ªç¯å±•å¼€ - ä½•æ—¶ä½¿ç”¨

### ğŸ“‹ åˆ¤æ–­æ ‡å‡†

#### âœ… åº”è¯¥ä½¿ç”¨å¾ªç¯å±•å¼€çš„æƒ…å†µ

**1. å¾ªç¯æ¬¡æ•°æ˜¯ç¼–è¯‘æ—¶å¸¸é‡ä¸”è¾ƒå°**

```cpp
// âœ… é€‚åˆå±•å¼€ï¼ˆæ¬¡æ•°å°ä¸”å›ºå®šï¼‰
#pragma unroll
for (int i = 0; i < 8; i++) {
    sum += data[i];
}

// âŒ ä¸é€‚åˆå±•å¼€ï¼ˆæ¬¡æ•°å¤§ï¼‰
#pragma unroll
for (int i = 0; i < 1000; i++) {  // å¤ªå¤šäº†
    sum += data[i];
}

// âŒ ä¸é€‚åˆå±•å¼€ï¼ˆæ¬¡æ•°åŠ¨æ€ï¼‰
for (int i = 0; i < N; i++) {  // N æ˜¯å˜é‡
    sum += data[i];
}
```

**2. å¾ªç¯ä½“ç®€å•ï¼Œæ²¡æœ‰åˆ†æ”¯**

```cpp
// âœ… é€‚åˆå±•å¼€
#pragma unroll
for (int i = 0; i < 4; i++) {
    result[i] = data[i] * 2.0f + 1.0f;  // ç®€å•æ“ä½œ
}

// âŒ ä¸é€‚åˆå±•å¼€
#pragma unroll
for (int i = 0; i < 4; i++) {
    if (data[i] > threshold) {  // æœ‰åˆ†æ”¯
        result[i] = expensive_compute(data[i]);
    }
}
```

**3. ILP ä¸è¶³**

```bash
ncu --section ComputeWorkloadAnalysis

çœ‹åˆ°ï¼š
smsp__issue_active < 65%

ä¸”å¾ªç¯ä½“æœ‰ç‹¬ç«‹æ“ä½œï¼š
â†’ å±•å¼€å¯ä»¥æé«˜ ILP
```

#### âŒ ä¸åº”è¯¥ä½¿ç”¨å¾ªç¯å±•å¼€çš„æƒ…å†µ

**1. ä¼šå¯¼è‡´å¯„å­˜å™¨æº¢å‡º**

```bash
nvcc --ptxas-options=-v kernel.cu

Before unroll:
ptxas info : Used 80 registers

After unroll:
ptxas info : Used 220 registers  // å¤ªå¤šï¼
ptxas info : Spilled registers to local memory  // æº¢å‡ºï¼

â†’ ä¸è¦å±•å¼€ï¼Œä¼šé™ä½æ€§èƒ½
```

**2. å¾ªç¯ä½“å¾ˆå¤§**

```cpp
// âŒ ä¸è¦å±•å¼€
#pragma unroll
for (int i = 0; i < 16; i++) {
    // 100 è¡Œä»£ç 
    complex_computation();
}
// å±•å¼€åä»£ç ä¼šéå¸¸å¤§ï¼Œå½±å“ I-cache
```

**3. å·²ç»æœ‰å¾ˆå¥½çš„ ILP**

```bash
ncu --metrics smsp__issue_active

å¦‚æœå·²ç» > 75%ï¼š
â†’ å±•å¼€æ”¶ç›Šå¾ˆå°
â†’ ä¸å€¼å¾—
```

### ğŸ“Š å¾ªç¯å±•å¼€æ•ˆæœ

| åœºæ™¯ | ä¸å±•å¼€ | å±•å¼€ | æå‡ |
|------|-------|-----|------|
| å°å¾ªç¯ï¼ˆ4-8æ¬¡ï¼‰ | 10 ms | 8.5 ms | 1.18x |
| ä¸­å¾ªç¯ï¼ˆ16-32æ¬¡ï¼‰ | 20 ms | 18 ms | 1.11x |
| å¤§å¾ªç¯ï¼ˆ>64æ¬¡ï¼‰ | 50 ms | 52 ms | 0.96xï¼ˆé™ä½ï¼ï¼‰ |
| å¤æ‚å¾ªç¯ä½“ | 30 ms | 35 ms | 0.86xï¼ˆé™ä½ï¼ï¼‰ |

### ğŸ’¡ å¾ªç¯å±•å¼€å®è·µæŒ‡å—

```cpp
// æ­¥éª¤ 1ï¼šå†³å®šæ˜¯å¦å±•å¼€
// æ£€æŸ¥ï¼šå¾ªç¯æ¬¡æ•° < 32ï¼Ÿå¾ªç¯ä½“ç®€å•ï¼Ÿå¯„å­˜å™¨å¤Ÿç”¨ï¼Ÿ

// æ­¥éª¤ 2ï¼šä½¿ç”¨ #pragma unroll
// è‡ªåŠ¨å±•å¼€
#pragma unroll
for (int i = 0; i < 8; i++) {
    result += data[i];
}

// éƒ¨åˆ†å±•å¼€
#pragma unroll 4
for (int i = 0; i < N; i++) {
    // æ¯ 4 æ¬¡å±•å¼€ä¸€æ¬¡
}

// æ­¥éª¤ 3ï¼šæˆ–æ‰‹åŠ¨å±•å¼€
// è‡ªåŠ¨å±•å¼€
for (int i = 0; i < 4; i++) {
    sum += data[i];
}

// æ‰‹åŠ¨å±•å¼€
sum += data[0];
sum += data[1];
sum += data[2];
sum += data[3];

// æ­¥éª¤ 4ï¼šéªŒè¯
nvcc --ptxas-options=-v kernel.cu
// æ£€æŸ¥å¯„å­˜å™¨ä½¿ç”¨

ncu --metrics smsp__issue_active
// æ£€æŸ¥æ˜¯å¦æå‡
```

---

## Tensor Cores - ä½•æ—¶ä½¿ç”¨

### ğŸ“‹ åˆ¤æ–­æ ‡å‡†

#### âœ… åº”è¯¥ä½¿ç”¨ Tensor Cores çš„æƒ…å†µ

**1. çŸ©é˜µä¹˜æ³•æ“ä½œ**

```cpp
// âœ… é€‚åˆ Tensor Cores
C = A Ã— B  // çŸ©é˜µä¹˜æ³•
C = A Ã— B + C  // å¸¦ç´¯åŠ çš„çŸ©é˜µä¹˜æ³•

// âŒ ä¸é€‚åˆ
C = A + B  // Element-wiseï¼ˆä¸æ˜¯çŸ©é˜µä¹˜æ³•ï¼‰
```

**2. ä½¿ç”¨ FP16, BF16, TF32, INT8 æ•°æ®ç±»å‹**

```
Tensor Core æ”¯æŒçš„æ•°æ®ç±»å‹ï¼š
- FP16 (æ‰€æœ‰ Tensor Core ä»£)
- BF16 (Ampere+)
- TF32 (Ampere+)
- INT8 (Turing+)
- FP64 (Ampere+)
- FP8 (Hopper+)
```

**NCU ç‰¹å¾**ï¼š
```bash
ncu --section ComputeWorkloadAnalysis

çœ‹åˆ°ï¼š
Compute-boundï¼ˆSM > 80%ï¼‰
å¤§é‡ FP16/FP32 çŸ©é˜µæ“ä½œ
ä½†ï¼š
smsp__inst_executed_pipe_tensor = 0%ï¼ˆæœªä½¿ç”¨ Tensor Coresï¼‰

â†’ åº”è¯¥ä½¿ç”¨ Tensor Cores
â†’ é¢„æœŸæå‡ï¼š5-20x
```

**3. çŸ©é˜µç»´åº¦æ˜¯ 8/16/32 çš„å€æ•°**

```
Tensor Core çŸ©é˜µå½¢çŠ¶ï¼š
- Volta/Turing: 16x16x16 (FP16)
- Ampere: 16x16x16 (FP16/BF16/TF32)
- Hopper: 16x16x16, è¿˜æ”¯æŒå…¶ä»–å½¢çŠ¶

æœ€ä½³ï¼šM, N, K éƒ½æ˜¯ 16 çš„å€æ•°
```

**4. çŸ©é˜µè¶³å¤Ÿå¤§**

```
Tensor Cores å¯¹å°çŸ©é˜µæ•ˆç‡ä½

æ¨èï¼š
- M, N, K >= 128
- æœ€å¥½ >= 512
```

#### âŒ ä¸åº”è¯¥ä½¿ç”¨ Tensor Cores çš„æƒ…å†µ

**1. ä¸æ˜¯çŸ©é˜µä¹˜æ³•**

```cpp
// âŒ è¿™äº›æ“ä½œä¸èƒ½ç”¨ Tensor Cores
- Element-wise æ“ä½œ
- Reduction
- Transpose
- å·ç§¯ï¼ˆéœ€è¦è½¬æ¢ä¸º GEMM æ ¼å¼ï¼‰
```

**2. çŸ©é˜µå¤ªå°**

```
å¦‚æœ M, N, K < 64ï¼š
â†’ Tensor Core å¯åŠ¨å¼€é”€ > æ”¶ç›Š
â†’ ä¸å¦‚ç”¨æ™®é€š CUDA Cores
```

**3. ç²¾åº¦è¦æ±‚ä¸¥æ ¼çš„ FP32**

```
TF32 è™½ç„¶å¿«ï¼Œä½†ç²¾åº¦ä½äº FP32
å¦‚æœéœ€è¦å®Œæ•´ FP32 ç²¾åº¦ï¼š
â†’ ä¸èƒ½ç”¨ TF32 Tensor Cores
â†’ ç”¨ FP32 CUDA Cores
```

### ğŸ“Š Tensor Core æ•ˆæœ

| åœºæ™¯ | CUDA Cores | Tensor Cores | æå‡ |
|------|-----------|-------------|------|
| MatMul FP16 (å¤§) | 15 TFLOPS | 200 TFLOPS | 13x |
| MatMul TF32 (å¤§) | 19 TFLOPS | 150 TFLOPS | 8x |
| MatMul FP32 (å¤§) | 19 TFLOPS | 19 TFLOPS | 1xï¼ˆæ— ç”¨ï¼‰ |
| MatMul FP16 (å°,64x64) | 2 TFLOPS | 5 TFLOPS | 2.5x |

### ğŸ’¡ Tensor Core å®è·µæŒ‡å—

```cpp
// æ­¥éª¤ 1ï¼šæ£€æŸ¥æ˜¯å¦é€‚åˆ
// - çŸ©é˜µä¹˜æ³•ï¼Ÿ
// - ä½¿ç”¨ FP16/TF32ï¼Ÿ
// - çŸ©é˜µå¤Ÿå¤§ï¼Ÿ

// æ­¥éª¤ 2ï¼šä½¿ç”¨ WMMA APIï¼ˆæ‰‹åŠ¨ï¼‰
#include <mma.h>
using namespace nvcuda;

__global__ void matmul_wmma(
    half* C, const half* A, const half* B,
    int M, int N, int K
) {
    // å£°æ˜ fragments
    wmma::fragment<wmma::matrix_a, 16, 16, 16, half, wmma::row_major> a_frag;
    wmma::fragment<wmma::matrix_b, 16, 16, 16, half, wmma::col_major> b_frag;
    wmma::fragment<wmma::accumulator, 16, 16, 16, half> c_frag;

    // åˆå§‹åŒ–
    wmma::fill_fragment(c_frag, 0.0f);

    // åŠ è½½
    wmma::load_matrix_sync(a_frag, A + ..., K);
    wmma::load_matrix_sync(b_frag, B + ..., K);

    // çŸ©é˜µä¹˜æ³•ï¼ˆä½¿ç”¨ Tensor Coresï¼‰
    wmma::mma_sync(c_frag, a_frag, b_frag, c_frag);

    // å­˜å‚¨
    wmma::store_matrix_sync(C + ..., c_frag, N, wmma::mem_row_major);
}

// æˆ–ä½¿ç”¨ cuBLASï¼ˆæ¨èï¼‰
cublasGemmEx(handle,
             CUBLAS_OP_N, CUBLAS_OP_N,
             M, N, K,
             &alpha,
             A, CUDA_R_16F, lda,
             B, CUDA_R_16F, ldb,
             &beta,
             C, CUDA_R_16F, ldc,
             CUDA_R_16F,
             CUBLAS_GEMM_DEFAULT_TENSOR_OP);  // ä½¿ç”¨ Tensor Cores

// æ­¥éª¤ 3ï¼šéªŒè¯
ncu --metrics smsp__inst_executed_pipe_tensor ./program

// åº”è¯¥çœ‹åˆ°ï¼š
// smsp__inst_executed_pipe_tensor > 0
// ä¸”æ€§èƒ½å¤§å¹…æå‡
```

---

## å®Œæ•´å†³ç­–æµç¨‹

### ç¬¬ä¸€æ­¥ï¼šè¯†åˆ«ç“¶é¢ˆç±»å‹

```bash
ncu --section SpeedOfLight ./program
```

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚    çœ‹ SM % å’Œ Memory %                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
              â†“
      â”Œâ”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”
      â†“               â†“
  SM > 80%        Memory > 80%
      â†“               â†“
 Compute-bound    Memory-bound
      â†“               â†“
   ç¬¬äºŒæ­¥A          ç¬¬äºŒæ­¥B
```

### ç¬¬äºŒæ­¥Aï¼šCompute-bound ä¼˜åŒ–å†³ç­–

```
Compute-bound æ£€æµ‹åˆ°ï¼
â”‚
â”œâ”€ 1. æ˜¯çŸ©é˜µä¹˜æ³•ä¸”ç”¨ FP16ï¼Ÿ
â”‚  â””â”€ âœ… â†’ ä½¿ç”¨ Tensor Coresï¼ˆ5-20xï¼‰
â”‚
â”œâ”€ 2. smsp__thread_inst_executed_per_inst_executed < 24ï¼Ÿ
â”‚  â””â”€ âœ… â†’ æ¶ˆé™¤ Warp Divergenceï¼ˆ1.5-3xï¼‰
â”‚
â”œâ”€ 3. smsp__issue_active < 60%ï¼Ÿ
â”‚  â””â”€ âœ… â†’ æé«˜ ILPï¼ˆ1.3-2xï¼‰
â”‚     - æ¯ä¸ªçº¿ç¨‹å¤„ç†å¤šä¸ªæ•°æ®
â”‚     - å¾ªç¯å±•å¼€ï¼ˆå¦‚æœå¾ªç¯å°ï¼‰
â”‚
â”œâ”€ 4. æœ‰è¶…è¶Šå‡½æ•°ï¼ˆexp, log, sinï¼‰ï¼Ÿ
â”‚  â””â”€ âœ… â†’ ä½¿ç”¨å¿«é€Ÿæ•°å­¦å‡½æ•°ï¼ˆ1.5-3xï¼‰
â”‚
â””â”€ 5. å¾ªç¯æ¬¡æ•°å°ï¼ˆ<16ï¼‰ä¸”å›ºå®šï¼Ÿ
   â””â”€ âœ… â†’ å¾ªç¯å±•å¼€ï¼ˆ1.1-1.3xï¼‰
```

### ç¬¬äºŒæ­¥Bï¼šMemory-bound ä¼˜åŒ–å†³ç­–

```
Memory-bound æ£€æµ‹åˆ°ï¼
â”‚
â”œâ”€ 1. æœ‰å¤šä¸ªï¼ˆ>3ï¼‰è¿ç»­çš„å° kernelï¼Ÿ
â”‚  â””â”€ âœ… â†’ ç®—å­èåˆï¼ˆ3-10xï¼‰â˜…æœ€ä¼˜å…ˆ
â”‚
â”œâ”€ 2. åŒä¸€æ•°æ®è¢«å¤šæ¬¡è®¿é—®ï¼Ÿ
â”‚  â”‚  (L2 hit rate < 50%)
â”‚  â””â”€ âœ… â†’ ä½¿ç”¨ Shared Memoryï¼ˆ2-5xï¼‰
â”‚
â”œâ”€ 3. l1tex__average_t_sectors_per_request > 1.5ï¼Ÿ
â”‚  â””â”€ âœ… â†’ ä¿®å¤ Coalesced Accessï¼ˆ2-4xï¼‰
â”‚
â”œâ”€ 4. å·²ç» coalesced ä½†å¸¦å®½ < 80%ï¼Ÿ
â”‚  â””â”€ âœ… â†’ å‘é‡åŒ– float4ï¼ˆ1.5-2xï¼‰
â”‚
â”œâ”€ 5. l1tex__data_bank_conflicts > 0ï¼Ÿ
â”‚  â””â”€ âœ… â†’ æ¶ˆé™¤ Bank Conflictsï¼ˆ1.2-1.5xï¼‰
â”‚
â””â”€ 6. éƒ½åšå®Œäº†è¿˜ä¸å¤Ÿï¼Ÿ
   â””â”€ â†’ è€ƒè™‘ç®—æ³•çº§ä¼˜åŒ–ã€CUDA Graphs
```

### ç¬¬ä¸‰æ­¥ï¼šéªŒè¯å’Œè¿­ä»£

```bash
# ä¼˜åŒ–åé‡æ–° profile
ncu --section SpeedOfLight ./program_optimized

# å¯¹æ¯”
ncu-ui baseline.ncu-rep optimized.ncu-rep

# å¦‚æœè¿˜ä¸å¤Ÿï¼Œå›åˆ°ç¬¬ä¸€æ­¥
```

---

## å®æˆ˜æ¡ˆä¾‹

### æ¡ˆä¾‹ 1ï¼šä¼˜åŒ– Element-wise Kernel

**åˆå§‹çŠ¶æ€**ï¼š
```bash
ncu --section SpeedOfLight ./program

SM Throughput: 8%
Memory Throughput: 92%
Duration: 15 ms
```

**åˆ†æ**ï¼šMemory-bound

**ä¼˜åŒ–è·¯å¾„**ï¼š

```
ç¬¬ä¸€æ­¥ï¼šæ£€æŸ¥æ˜¯å¦æœ‰å¤šä¸ª kernel
nsys profile ./program

å‘ç°ï¼š
- relu_kernel: 5 ms
- add_bias_kernel: 5 ms
- scale_kernel: 5 ms
æ€»å…± 15 ms

å†³ç­–ï¼šâœ… ç®—å­èåˆï¼ˆä¼˜å…ˆçº§ 1ï¼‰

å®æ–½ï¼š
__global__ void fused_kernel(float* data, float bias, float scale, int N) {
    int idx = blockIdx.x * blockDim.x + threadIdx.x;
    if (idx < N) {
        float val = data[idx];
        val = fmaxf(0.0f, val);
        val += bias;
        val *= scale;
        data[idx] = val;
    }
}

ç»“æœï¼š
Duration: 15 ms â†’ 5 ms
åŠ é€Ÿæ¯”ï¼š3x âœ“
```

**ç»§ç»­ä¼˜åŒ–**ï¼š

```
ç¬¬äºŒæ­¥ï¼šæ£€æŸ¥ coalesced
ncu --section MemoryWorkloadAnalysis ./program

l1tex__average_t_sectors_per_request = 1.02 âœ“ï¼ˆå·²ç» coalescedï¼‰

ç¬¬ä¸‰æ­¥ï¼šæ£€æŸ¥å¸¦å®½
å®é™…å¸¦å®½ï¼š800 GB/s
ç†è®ºå¸¦å®½ï¼š1555 GB/s
åˆ©ç”¨ç‡ï¼š51%

å†³ç­–ï¼šâœ… å‘é‡åŒ–ï¼ˆä¼˜å…ˆçº§ 2ï¼‰

å®æ–½ float4ï¼š
ï¼ˆä»£ç è§å‰é¢å‘é‡åŒ–éƒ¨åˆ†ï¼‰

ç»“æœï¼š
Duration: 5 ms â†’ 3.2 ms
æ€»åŠ é€Ÿæ¯”ï¼š15/3.2 = 4.7x âœ“
```

### æ¡ˆä¾‹ 2ï¼šä¼˜åŒ– MatMul Kernel

**åˆå§‹çŠ¶æ€**ï¼š
```bash
ncu --section SpeedOfLight ./matmul

SM Throughput: 45%
Memory Throughput: 75%
Duration: 20 ms
```

**åˆ†æ**ï¼šBalancedï¼Œä½†éƒ½ä¸é«˜

**ä¼˜åŒ–è·¯å¾„**ï¼š

```
ç¬¬ä¸€æ­¥ï¼šæ£€æŸ¥ Occupancy
ncu --section Occupancy

Achieved Occupancy: 35%
Limiting Factor: Registers (200 per thread)

å†³ç­–ï¼šâœ… å…ˆæé«˜ Occupancy

å®æ–½ï¼š
__global__ void __launch_bounds__(256, 4)
matmul_kernel(...) {
    // é™åˆ¶å¯„å­˜å™¨ä½¿ç”¨
}

ç»“æœï¼š
Occupancy: 35% â†’ 62%
Duration: 20 ms â†’ 15 ms
```

**ç»§ç»­ä¼˜åŒ–**ï¼š

```
ç¬¬äºŒæ­¥ï¼šæ£€æŸ¥æ•°æ®å¤ç”¨
ncu --section MemoryWorkloadAnalysis

L2 hit rate: 25%ï¼ˆå¾ˆä½ï¼ï¼‰

å†³ç­–ï¼šâœ… ä½¿ç”¨ Shared Memory Tiling

å®æ–½ï¼š
ï¼ˆTiled MatMulä»£ç è§å‰é¢ï¼‰

ç»“æœï¼š
Duration: 15 ms â†’ 6 ms
```

**æœ€ç»ˆä¼˜åŒ–**ï¼š

```
ç¬¬ä¸‰æ­¥ï¼šæ£€æŸ¥æ˜¯å¦å¯ç”¨ Tensor Cores

æ¡ä»¶ï¼š
- çŸ©é˜µä¹˜æ³• âœ“
- å¯ä»¥ç”¨ FP16 âœ“
- çŸ©é˜µå¤§å°ï¼š2048x2048 âœ“

å†³ç­–ï¼šâœ… ä½¿ç”¨ Tensor Cores

å®æ–½ï¼š
ä½¿ç”¨ cuBLAS with Tensor Cores

ç»“æœï¼š
Duration: 6 ms â†’ 0.8 ms
æ€»åŠ é€Ÿæ¯”ï¼š20/0.8 = 25x âœ“
```

---

## ä¼˜åŒ–å†³ç­–é€ŸæŸ¥è¡¨

| åœºæ™¯ç‰¹å¾ | NCU æŒ‡æ ‡ | ä¼˜åŒ–æ–¹æ³• | ä¼˜å…ˆçº§ | é¢„æœŸæå‡ |
|---------|---------|---------|--------|---------|
| å¤šä¸ªå° kernel è¿ç»­ | Memory>80%, Duration<10ms | **ç®—å­èåˆ** | â­â­â­ | 3-10x |
| æ•°æ®å¤šæ¬¡è®¿é—® | L2 hit<50% | **Shared Memory** | â­â­â­ | 2-5x |
| çŸ©é˜µä¹˜æ³•+FP16 | SM>80%, FP16 ops | **Tensor Cores** | â­â­â­ | 5-20x |
| è·¨æ­¥è®¿é—® | sectors/request>1.5 | **ä¿®å¤ Coalesced** | â­â­â­ | 2-4x |
| Memory-bound+coalesced | Memory>80%, sectors=1.0 | **å‘é‡åŒ– float4** | â­â­ | 1.5-2x |
| æŒ‡ä»¤å‘å°„ä½ | issue_active<60% | **æé«˜ ILP** | â­â­ | 1.3-2x |
| Warp åˆ†æ”¯å¤š | threads/inst<24 | **æ¶ˆé™¤ Divergence** | â­â­ | 1.5-3x |
| å°å¾ªç¯å›ºå®š | - | **å¾ªç¯å±•å¼€** | â­ | 1.1-1.3x |
| Bank conflicts | conflicts>0 | **æ·»åŠ  padding** | â­â­ | 1.2-1.5x |
| Occupancy ä½ | Occupancy<40% | **è°ƒæ•´èµ„æº** | â­â­ | 1.2-2x |

**ä¼˜å…ˆçº§è¯´æ˜**ï¼š
- â­â­â­ï¼šå¿…åšï¼ˆå½±å“å¤§ï¼‰
- â­â­ï¼šæ¨èåšï¼ˆæœ‰æ˜æ˜¾æ•ˆæœï¼‰
- â­ï¼šå¯é€‰ï¼ˆé”¦ä¸Šæ·»èŠ±ï¼‰

---

## æ€»ç»“

### ä¼˜åŒ–å†³ç­–çš„é»„é‡‘æ³•åˆ™

1. **å…ˆçœ‹å¤§çš„ï¼Œå†çœ‹å°çš„**
   - ç®—å­èåˆï¼ˆ3-10xï¼‰> å•ä¸ª kernel ä¼˜åŒ–ï¼ˆ1.5-3xï¼‰

2. **å…ˆè§£å†³ç“¶é¢ˆï¼Œå†åšä¼˜åŒ–**
   - Memory-bound â†’ ä¸åšè®¡ç®—ä¼˜åŒ–
   - Compute-bound â†’ ä¸åšå‘é‡åŒ–

3. **ä¼˜å…ˆåšæ”¶ç›Šå¤§çš„**
   - Tensor Coresï¼ˆ5-20xï¼‰> ILPï¼ˆ1.3-2xï¼‰> å¾ªç¯å±•å¼€ï¼ˆ1.1-1.3xï¼‰

4. **æ¯æ¬¡ä¼˜åŒ–åéƒ½éªŒè¯**
   - é˜²æ­¢ä¼˜åŒ–é€‚å¾—å…¶å

5. **ä¸è¦è¿‡æ—©ä¼˜åŒ–**
   - å…ˆ profileï¼Œç¡®è®¤ç“¶é¢ˆ
   - ä¸è¦çŒœæµ‹

### å¿«é€Ÿå†³ç­–æµç¨‹

```
1. ncu --section SpeedOfLight
   â†’ åˆ¤æ–­ Compute è¿˜æ˜¯ Memory bound

2. å¦‚æœ Memory-boundï¼š
   â†’ å…ˆçœ‹æœ‰æ²¡æœ‰å¤šä¸ª kernelï¼ˆèåˆï¼‰
   â†’ å†çœ‹ L2 hit rateï¼ˆShared Memoryï¼‰
   â†’ å†çœ‹ coalescedï¼ˆä¿®å¤è®¿é—®æ¨¡å¼ï¼‰
   â†’ æœ€åå‘é‡åŒ–

3. å¦‚æœ Compute-boundï¼š
   â†’ å…ˆçœ‹æ˜¯å¦çŸ©é˜µä¹˜æ³•ï¼ˆTensor Coresï¼‰
   â†’ å†çœ‹ divergenceï¼ˆæ¶ˆé™¤åˆ†æ”¯ï¼‰
   â†’ å†çœ‹ ILPï¼ˆæé«˜å¹¶è¡Œï¼‰

4. éªŒè¯ï¼Œè¿­ä»£
```
