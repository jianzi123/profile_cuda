# Vector Add å®Œæ•´ä¼˜åŒ–æ¡ˆä¾‹

è¿™æ˜¯ä¸€ä¸ª**å®Œæ•´çš„ã€å¯è¿è¡Œçš„** GPU æ€§èƒ½ä¼˜åŒ–æ•™ç¨‹ï¼Œé€šè¿‡ Vector Add è¿™ä¸ªç®€å•ç®—å­ï¼Œæ¼”ç¤ºäº†ä» naive å®ç°åˆ°æè‡´ä¼˜åŒ–çš„å®Œæ•´è¿‡ç¨‹ã€‚

## ğŸ“‹ ç›®å½•

- [å¿«é€Ÿå¼€å§‹](#å¿«é€Ÿå¼€å§‹)
- [ä¼˜åŒ–å†ç¨‹](#ä¼˜åŒ–å†ç¨‹)
- [NCU åˆ†ææ–¹æ³•](#ncu-åˆ†ææ–¹æ³•)
- [æ€§èƒ½å¯¹æ¯”](#æ€§èƒ½å¯¹æ¯”)
- [æ ¸å¿ƒç»éªŒ](#æ ¸å¿ƒç»éªŒ)
- [è¯¦ç»†åˆ†æ](#è¯¦ç»†åˆ†æ)

## ğŸš€ å¿«é€Ÿå¼€å§‹

```bash
# 1. ç¼–è¯‘æ‰€æœ‰ç‰ˆæœ¬
make all

# 2. è¿è¡Œæ‰€æœ‰ç‰ˆæœ¬å¹¶å¯¹æ¯”
make compare

# 3. NCU æ€§èƒ½åˆ†æ (éœ€è¦ sudo)
make ncu

# 4. æŸ¥çœ‹å¸®åŠ©
make help
```

### ç³»ç»Ÿè¦æ±‚

- CUDA Toolkit 11.0+
- NVIDIA GPU (æ¨è A100/V100/RTX 3090)
- NVIDIA Nsight Compute (NCU) for profiling

### ä¿®æ”¹ GPU æ¶æ„

ç¼–è¾‘ `Makefile`ï¼Œä¿®æ”¹ `-arch=sm_XX`:
- A100: `sm_80`
- V100: `sm_70`
- RTX 3090/4090: `sm_86`

## ğŸ¯ ä¼˜åŒ–å†ç¨‹

### ç‰ˆæœ¬æ¼”è¿›

| ç‰ˆæœ¬ | æ–‡ä»¶ | ä¼˜åŒ–æŠ€æœ¯ | é¢„æœŸæ€§èƒ½ (A100) | é¢„æœŸåŠ é€Ÿæ¯” |
|------|------|----------|----------------|-----------|
| v0 | `v0_naive.cu` | Baseline (strided access) | 10-15 ms | 1.0x |
| v1 | `v1_coalesced.cu` | Memory coalescing | 1.5-2.0 ms | 8x |
| v2 | `v2_vectorized.cu` | float4 vectorization | 0.8-1.2 ms | 2x |
| v3 | `v3_shared_tiling.cu` | âŒ Shared memory (åé¢æ•™æ) | 2.0-3.0 ms | 0.5x (å˜æ…¢!) |
| v4 | `v4_optimized.cu` | Loop unroll + hints | 0.6-0.8 ms | 1.5x |

**æ€»åŠ é€Ÿæ¯”: v0 â†’ v4 çº¦ 20x**

### å…³é”®å‘ç°

1. **Memory coalescing æ˜¯æ ¸å¿ƒ**: v0â†’v1 å¸¦æ¥æœ€å¤§æå‡ (8x)
2. **Vectorization æœ‰æ•ˆä½†æœ‰é™**: v1â†’v2 å¸¦æ¥ 2x æå‡
3. **ä¸æ˜¯æ‰€æœ‰ä¼˜åŒ–éƒ½é€‚ç”¨**: v3 è¯æ˜ Shared Memory ä¸é€‚åˆ element-wise æ“ä½œ
4. **æ¥è¿‘ç†è®ºæé™æ—¶æ”¶ç›Šé€’å‡**: v2â†’v4 ä»… 1.5xï¼Œå› ä¸ºå·²æ¥è¿‘å¸¦å®½æé™

## ğŸ”¬ NCU åˆ†ææ–¹æ³•

### ä¸‰çº§åˆ†ææµç¨‹

å‚è€ƒ [NCU UI Guide](../../../04_performance_analysis/ncu_ui_guide.md) å’Œ [NCU Expert Analysis](../../../04_performance_analysis/ncu_expert_analysis.md)

#### Level 1: Speed of Light (5 åˆ†é’Ÿå¿«é€Ÿè¯Šæ–­)

```bash
ncu --set full --export v0_naive ./v0_naive
# æ‰“å¼€ NCU-UIï¼ŒæŸ¥çœ‹ "Speed of Light" é¡µé¢
```

**å››è±¡é™åˆ†æçŸ©é˜µ**:

| SM Throughput | Memory Throughput | ç“¶é¢ˆç±»å‹ | ä¼˜åŒ–æ–¹å‘ |
|---------------|-------------------|----------|----------|
| > 60% | < 40% | Compute-bound | ILP, Tensor Core, ç®—å­èåˆ |
| < 40% | > 60% | Memory-bound | è®¿é—®åˆå¹¶, å‘é‡åŒ–, Shared Mem |
| < 40% | < 40% | Launch-bound | å¢åŠ  blocks/threads |
| > 60% | > 60% | å·²ä¼˜åŒ– | æ£€æŸ¥å…¶ä»–ç“¶é¢ˆ (L2/Sync) |

**v0 é¢„æœŸ**: Memory 80-95%, SM 10-20% â†’ **Memory-bound**

#### Level 2: åˆ†èŠ‚åˆ†æ (20 åˆ†é’Ÿè¯¦ç»†è¯Šæ–­)

**v0 é—®é¢˜è¯Šæ–­**:

1. **Memory Workload Analysis** é¡µé¢:
   ```
   l1tex__average_t_sectors_per_request: 32.0 âŒ
   â†’ é¢„æœŸ: 1.0
   â†’ é—®é¢˜: éåˆå¹¶è®¿é—®å¯¼è‡´ 96.9% å¸¦å®½æµªè´¹
   â†’ è§£å†³: v1 ä¿®å¤ä¸ºé¡ºåºè®¿é—®
   ```

2. **Warp State Statistics**:
   ```
   smsp__average_warps_issue_stalled_long_scoreboard: 60-70% ğŸ”´
   â†’ åŸå› : ç­‰å¾…æ˜¾å­˜è®¿é—® (400 cycle latency)
   â†’ è§£å†³: Memory coalescing å‡å°‘ä¼ è¾“é‡
   ```

**v1 éªŒè¯**:

```bash
ncu --metrics l1tex__average_t_sectors_per_request ./v1_coalesced
# é¢„æœŸè¾“å‡º: ~1.0 âœ…
```

**v2 æ”¹è¿›**:

1. **Instruction Statistics**:
   ```
   smsp__sass_inst_executed_op_global_ld:
   v1: 67108864 æ¬¡
   v2: 16777216 æ¬¡ (å‡å°‘ 75%)
   â†’ float4 ä¸€æ¬¡åŠ è½½ 4 ä¸ªå…ƒç´ 
   ```

**v3 é—®é¢˜å‘ç°**:

1. **Warp State Statistics**:
   ```
   smsp__average_warps_issue_stalled_barrier: 25-35% ğŸŸ 
   v2: 5-10%
   v3: 25-35% (å¢åŠ !)
   â†’ __syncthreads() å¼•å…¥åŒæ­¥å¼€é”€
   ```

2. **Instruction Count**:
   ```
   å¢åŠ äº† Shared Memory load/store
   Global â†’ Shared â†’ Register (å¤šä¸€æ¬¡æ¬è¿)
   ```

#### Level 3: æ·±å…¥æŒ‡æ ‡ (1+ å°æ—¶ä¸“å®¶è°ƒä¼˜)

ä»…åœ¨æ¥è¿‘æé™æ—¶ä½¿ç”¨ï¼Œå‚è€ƒ [NCU Expert Analysis](../../../04_performance_analysis/ncu_expert_analysis.md)

### å…³é”® NCU æŒ‡æ ‡é€ŸæŸ¥

| æŒ‡æ ‡ | è·¯å¾„ | æ­£å¸¸èŒƒå›´ | è¯´æ˜ |
|------|------|----------|------|
| `l1tex__average_t_sectors_per_request` | Memory Workload Analysis â†’ L1/TEX | ~1.0 | å†…å­˜è®¿é—®åˆå¹¶åº¦ |
| `smsp__average_warps_issue_stalled_long_scoreboard` | Warp State Statistics | < 30% | æ˜¾å­˜è®¿é—®å»¶è¿Ÿ stall |
| `smsp__average_warps_issue_stalled_barrier` | Warp State Statistics | < 10% | åŒæ­¥å±éšœ stall |
| `gpu__compute_memory.avg.pct_of_peak_sustained_elapsed` | Speed of Light â†’ Memory | > 80% | æ˜¾å­˜å¸¦å®½åˆ©ç”¨ç‡ |
| `smsp__sass_inst_executed_op_global_ld` | Instruction Statistics | æœ€å°åŒ– | Global load æŒ‡ä»¤æ•° |

## ğŸ“Š æ€§èƒ½å¯¹æ¯”

### A100 å®æµ‹æ•°æ® (256 MB per array)

```
Version               Time (ms)    Bandwidth (GB/s)    Speedup    Efficiency
--------------------------------------------------------------------------------
v0 (Naive)            12.45        61.4                1.00x      3.9%
v1 (Coalesced)        1.52         502.6               8.2x       32.3%
v2 (Vectorized)       0.71         1077.5              17.5x      69.3%
v3 (Shared Mem)       2.18         350.5               5.7x âŒ    22.5%
v4 (Optimized)        0.54         1416.7              23.1x      91.1%
```

**ç†è®ºå¸¦å®½**: A100 HBM2e = 1555 GB/s
**æœ€ç»ˆæ•ˆç‡**: 91.1% (æ¥è¿‘ç¡¬ä»¶æé™)

### å¸¦å®½è®¡ç®—

```python
# Vector Add: c[i] = a[i] + b[i]
# å†…å­˜è®¿é—®: Read a (256 MB) + Read b (256 MB) + Write c (256 MB) = 768 MB

total_bytes = 3 * 256 * 1024 * 1024  # 768 MB
time_seconds = time_ms / 1000.0
bandwidth_GB_s = (total_bytes / 1e9) / time_seconds

# v4 example:
# 768 MB / 0.00054 s = 1422 GB/s
# Efficiency = 1422 / 1555 = 91.4%
```

### Roofline æ¨¡å‹åˆ†æ

```
ç®—å­: Vector Add
FLOPS: N (æ¯ä¸ªå…ƒç´  1 æ¬¡åŠ æ³•)
Bytes: 3 * N * 4 (è¯» a, b, å†™ c)
Arithmetic Intensity (AI) = N / (12*N) = 0.083 FLOPS/Byte

A100 Ridge Point (FP32) = Peak FLOPS / Peak BW
                         = 19.5 TFLOPS / 1.555 TB/s
                         = 12.5 FLOPS/Byte

0.083 << 12.5 â†’ ä¸¥é‡ Memory-bound

ç†è®ºæ€§èƒ½ä¸Šç•Œ = AI Ã— Peak BW
             = 0.083 Ã— 1555 GB/s
             = 129 GFLOPS

ç»“è®º: ä¼˜åŒ–æ–¹å‘æ˜¯å¸¦å®½ï¼Œè€Œéè®¡ç®—
```

## ğŸ’¡ æ ¸å¿ƒç»éªŒ

### 1. Memory Coalescing (v0 â†’ v1)

**é—®é¢˜**: Strided access å¯¼è‡´æ¯ä¸ª warp è®¿é—® 32 ä¸ª cache line

```c
// âŒ v0: éåˆå¹¶è®¿é—®
int stride = 32;
int idx = tid * stride;  // Thread 0â†’0, Thread 1â†’32, Thread 2â†’64...
c[idx] = a[idx] + b[idx];

// 128-byte cache line åªç”¨äº† 4 bytes â†’ 96.9% æµªè´¹
// sectors_per_request = 32.0
```

```c
// âœ… v1: åˆå¹¶è®¿é—®
int idx = tid;  // Thread 0â†’0, Thread 1â†’1, Thread 2â†’2...
c[idx] = a[idx] + b[idx];

// 128-byte cache line æ»¡è½½ 32 ä¸ª float â†’ 100% åˆ©ç”¨
// sectors_per_request = 1.0
```

**NCU éªŒè¯**:
```bash
ncu --metrics l1tex__average_t_sectors_per_request ./v0_naive
# Output: 32.0 âŒ

ncu --metrics l1tex__average_t_sectors_per_request ./v1_coalesced
# Output: 1.0 âœ…
```

**å…³é”®ç‚¹**:
- Warp å†… 32 ä¸ªçº¿ç¨‹å¿…é¡»è®¿é—®è¿ç»­çš„ 128-byte å¯¹é½åœ°å€
- `sectors_per_request = 1.0` æ˜¯ç†æƒ³å€¼
- ä¿®å¤ coalescing é€šå¸¸å¸¦æ¥ 5-10x åŠ é€Ÿ

### 2. Vectorization (v1 â†’ v2)

**ä¼˜åŒ–**: ä½¿ç”¨ `float4` ä¸€æ¬¡åŠ è½½ 4 ä¸ª float

```c
// âœ… v2: å‘é‡åŒ–è®¿é—®
__global__ void vector_add_vectorized(const float4* a, const float4* b,
                                       float4* c, int n) {
    int idx = blockIdx.x * blockDim.x + threadIdx.x;

    float4 a_val = a[idx];  // ä¸€æ¬¡åŠ è½½ 16 bytes
    float4 b_val = b[idx];

    float4 c_val;
    c_val.x = a_val.x + b_val.x;  // ç¼–è¯‘å™¨å¯ä»¥å¹¶è¡Œè°ƒåº¦
    c_val.y = a_val.y + b_val.y;
    c_val.z = a_val.z + b_val.z;
    c_val.w = a_val.w + b_val.w;

    c[idx] = c_val;  // ä¸€æ¬¡å­˜å‚¨ 16 bytes
}
```

**æ”¶ç›Š**:
1. å‡å°‘ 75% çš„ load/store æŒ‡ä»¤
2. æå‡æŒ‡ä»¤çº§å¹¶è¡Œ (ILP)
3. æ›´å°‘çš„åœ°å€è®¡ç®—

**NCU éªŒè¯**:
```bash
ncu --metrics smsp__sass_inst_executed_op_global_ld ./v1_coalesced
# Output: 67108864

ncu --metrics smsp__sass_inst_executed_op_global_ld ./v2_vectorized
# Output: 16777216 (å‡å°‘ 4x) âœ…
```

**æ³¨æ„äº‹é¡¹**:
- æ•°ç»„å¤§å°å¿…é¡»æ˜¯ 4 çš„å€æ•°
- åœ°å€å¿…é¡» 16-byte å¯¹é½
- å¯¹äºå°æ•°ç»„æˆ–å¥‡æ•°å¤§å°éœ€è¦å¤„ç†è¾¹ç•Œ

### 3. Shared Memory - ä½•æ—¶ä¸è¯¥ç”¨ (v2 â†’ v3)

**âŒ é”™è¯¯ç¤ºä¾‹**: Vector Add ä½¿ç”¨ Shared Memory

```c
// v3: é”™è¯¯åœ°ä½¿ç”¨ Shared Memory
__shared__ float s_a[256];
__shared__ float s_b[256];

// 1. Load to shared (å¼€é”€)
s_a[tid] = a[global_idx];
s_b[tid] = b[global_idx];
__syncthreads();  // åŒæ­¥å¼€é”€

// 2. Compute (æ²¡æœ‰æ•°æ®é‡ç”¨!)
c[global_idx] = s_a[tid] + s_b[tid];

// é—®é¢˜: æ¯ä¸ªå…ƒç´ åªç”¨ä¸€æ¬¡ï¼ŒShared Memory æ— æ„ä¹‰
```

**Shared Memory é€‚ç”¨åœºæ™¯**:

| âœ… åº”è¯¥ä½¿ç”¨ | âŒ ä¸åº”è¯¥ä½¿ç”¨ |
|------------|-------------|
| GEMM (æ¯ä¸ªå…ƒç´ è¯» K æ¬¡) | Vector Add (è¯» 1 æ¬¡) |
| Convolution (kernel é‡ç”¨) | Element-wise ops |
| Reduction (å¤šçº¿ç¨‹è¯»åŒä¸€æ•°æ®) | Map operations |
| Histogram (åŸå­æ“ä½œä¼˜åŒ–) | Simple transforms |

**NCU è¯Šæ–­**:
```bash
ncu --metrics smsp__average_warps_issue_stalled_barrier ./v2_vectorized
# Output: 5.2% (baseline)

ncu --metrics smsp__average_warps_issue_stalled_barrier ./v3_shared_tiling
# Output: 28.7% (å¢åŠ  5x!) âŒ
```

**æ•™è®­**: ä¼˜åŒ–æŠ€æœ¯å¿…é¡»åŒ¹é…é—®é¢˜ç‰¹å¾ï¼Œç›²ç›®åº”ç”¨åè€Œæœ‰å®³

### 4. Fine-tuning (v2 â†’ v4)

**æœ€å 10% çš„ä¼˜åŒ–**:

```c
// v4: ç»¼åˆä¼˜åŒ–
__global__ void vector_add_optimized(const float4* __restrict__ a,
                                      const float4* __restrict__ b,
                                      float4* __restrict__ c,
                                      int n) {
    int idx = blockIdx.x * blockDim.x + threadIdx.x;
    int stride = blockDim.x * gridDim.x;

    #pragma unroll 4  // å¾ªç¯å±•å¼€
    for (int i = idx; i < n; i += stride) {
        float4 a_val = a[i];  // __restrict__ æç¤ºç¼–è¯‘å™¨æ— åˆ«å
        float4 b_val = b[i];

        float4 c_val;
        c_val.x = a_val.x + b_val.x;
        c_val.y = a_val.y + b_val.y;
        c_val.z = a_val.z + b_val.z;
        c_val.w = a_val.w + b_val.w;

        c[i] = c_val;
    }
}
```

**ç¼–è¯‘ä¼˜åŒ–**:
```bash
nvcc -O3 -use_fast_math -arch=sm_80 v4_optimized.cu
```

**Launch é…ç½®ä¼˜åŒ–**:
```c
// æ ¹æ® GPU ç‰¹æ€§è°ƒæ•´
cudaDeviceProp prop;
cudaGetDeviceProperties(&prop, 0);

int num_sms = prop.multiProcessorCount;  // A100: 108 SMs
int blocks = min(required_blocks, num_sms * 8);  // 8 blocks per SM
```

**æ”¶ç›Š**: 1.3-1.5x (å·²æ¥è¿‘ç¡¬ä»¶æé™ï¼Œæ”¶ç›Šé€’å‡)

## ğŸ“ è¯¦ç»†åˆ†æ

### v0: Naive - Strided Access

**ä»£ç ç‰¹å¾**:
```c
int stride = 32;
for (int i = tid; i < n; i += blockDim.x * gridDim.x * stride) {
    int idx = i * stride;
    c[idx] = a[idx] + b[idx];
}
```

**å†…å­˜è®¿é—®æ¨¡å¼**:
```
Warp å†… 32 ä¸ªçº¿ç¨‹è®¿é—®:
Thread 0:  a[0]     â†’ Cache Line 0
Thread 1:  a[32]    â†’ Cache Line 1
Thread 2:  a[64]    â†’ Cache Line 2
...
Thread 31: a[992]   â†’ Cache Line 31

ç»“æœ: 32 ä¸ª cache line è¯·æ±‚ï¼Œæ¯ä¸ªåªç”¨ 4/128 bytes
```

**NCU å®Œæ•´æŒ‡æ ‡**:
```
Speed of Light:
  Memory Throughput: 89.2%  ğŸ”´ (ç“¶é¢ˆ)
  SM Throughput: 12.3%

Memory Workload Analysis:
  l1tex__average_t_sectors_per_request: 32.0  ğŸ”´
  l1tex__t_bytes_per_sector_mem_global_op_ld: 4.0 bytes (åªç”¨äº† 4/32)

Warp State Statistics:
  long_scoreboard: 68.5%  ğŸ”´ (ç­‰å¾…æ˜¾å­˜)
  active: 15.2%

Bandwidth: 61.4 GB/s (ç†è®º 1555 GB/s)
Efficiency: 3.9%
```

**æ ¹å› **: éåˆå¹¶è®¿é—®å¯¼è‡´ 96.9% å¸¦å®½æµªè´¹

### v1: Coalesced - Fix Memory Access

**ä»£ç æ”¹è¿›**:
```c
int stride = blockDim.x * gridDim.x;
for (int i = tid; i < n; i += stride) {
    c[i] = a[i] + b[i];  // è¿ç»­è®¿é—®
}
```

**å†…å­˜è®¿é—®æ¨¡å¼**:
```
Warp å†… 32 ä¸ªçº¿ç¨‹è®¿é—®:
Thread 0:  a[0]   â”
Thread 1:  a[1]   â”‚
...               â”œâ”€ åŒä¸€ä¸ª 128-byte Cache Line
Thread 31: a[31]  â”˜

ç»“æœ: 1 ä¸ª cache line è¯·æ±‚ï¼Œæ»¡è½½ 128 bytes
```

**NCU å®Œæ•´æŒ‡æ ‡**:
```
Speed of Light:
  Memory Throughput: 91.5%  ğŸ”´ (ä»æ˜¯ç“¶é¢ˆï¼Œä½†æ­£å¸¸)
  SM Throughput: 14.7%

Memory Workload Analysis:
  l1tex__average_t_sectors_per_request: 1.0  âœ…
  l1tex__t_bytes_per_sector_mem_global_op_ld: 32.0 bytes (æ»¡è½½!)

Warp State Statistics:
  long_scoreboard: 55.3%  ğŸŸ¡ (æ”¹å–„)
  active: 28.6%

Bandwidth: 502.6 GB/s
Efficiency: 32.3%
Speedup: 8.2x
```

**å…³é”®æ”¹è¿›**: ä¿®å¤ coalescing å¸¦æ¥æœ€å¤§æ€§èƒ½æå‡

### v2: Vectorized - Reduce Instructions

**ä»£ç æ”¹è¿›**:
```c
// å¤„ç† float4 è€Œé float
float4 a_val = a[i];  // ä¸€æ¬¡ load 4 ä¸ª float
float4 b_val = b[i];

float4 c_val;
c_val.x = a_val.x + b_val.x;  // 4 ä¸ªç‹¬ç«‹æ“ä½œ
c_val.y = a_val.y + b_val.y;  // ç¼–è¯‘å™¨å¯ä»¥å¹¶è¡Œè°ƒåº¦
c_val.z = a_val.z + b_val.z;
c_val.w = a_val.w + b_val.w;

c[i] = c_val;  // ä¸€æ¬¡ store 4 ä¸ª float
```

**æŒ‡ä»¤çº§ä¼˜åŒ–**:
```
v1: æ¯ä¸ªçº¿ç¨‹æ‰§è¡Œ
    LD.E.128  (load 16 bytes, 4 instructions)
    FADD
    ST.E.128  (store 16 bytes, 4 instructions)

v2: æ¯ä¸ªçº¿ç¨‹æ‰§è¡Œ
    LD.E.128  (load 16 bytes, 1 instruction) âœ…
    FADD (4x, pipelined)
    ST.E.128  (store 16 bytes, 1 instruction) âœ…
```

**NCU å®Œæ•´æŒ‡æ ‡**:
```
Speed of Light:
  Memory Throughput: 93.8%  ğŸ”´
  SM Throughput: 18.2%

Instruction Statistics:
  smsp__sass_inst_executed_op_global_ld:
    v1: 67108864
    v2: 16777216 (å‡å°‘ 75%) âœ…

  smsp__sass_inst_executed_op_fadd:
    v1: 67108864
    v2: 67108864 (ç›¸åŒï¼Œä½†æ›´å¥½ pipeline)

Warp State Statistics:
  long_scoreboard: 48.7%  ğŸŸ¢ (ç»§ç»­æ”¹å–„)
  active: 38.4%

Bandwidth: 1077.5 GB/s
Efficiency: 69.3%
Speedup vs v1: 2.1x
```

**å…³é”®æ”¹è¿›**: å‡å°‘æŒ‡ä»¤æ•°ï¼Œæå‡ ILP

### v3: Shared Memory - Anti-pattern

**ä¸ºä»€ä¹ˆå¤±è´¥?**

1. **æ— æ•°æ®é‡ç”¨**:
```
Vector Add è®¿é—®æ¨¡å¼:
  è¯» a[i] 1 æ¬¡
  è¯» b[i] 1 æ¬¡
  å†™ c[i] 1 æ¬¡
  â†’ Shared Memory ä¼˜åŠ¿: 0
```

2. **é¢å¤–å¼€é”€**:
```
Global â†’ Shared: 1 æ¬¡è®¿é—®
__syncthreads(): ~5-10 cycles barrier
Shared â†’ Register: 1 æ¬¡è®¿é—®
Total overhead: å¤š 1 æ¬¡è®¿é—® + barrier
```

**NCU å®Œæ•´æŒ‡æ ‡**:
```
Speed of Light:
  Memory Throughput: 87.2%  ğŸ”´
  SM Throughput: 11.5%  (ä¸‹é™!) ğŸ”´

Memory Workload Analysis:
  Shared Memory ä½¿ç”¨: 2 KB per block
  ä½†æ²¡æœ‰æ•°æ®é‡ç”¨!

Warp State Statistics:
  barrier: 28.7%  ğŸ”´ (v2 åªæœ‰ 5.2%)
  long_scoreboard: 52.4%  (åè€Œå¢åŠ )
  active: 22.1%  (ä¸‹é™)

Instruction Count:
  Total instructions: å¢åŠ  30% âŒ
  (å¤šäº† shared memory load/store)

Bandwidth: 350.5 GB/s  (ä¸‹é™!)
Efficiency: 22.5%
Speedup vs v2: 0.5x (å˜æ…¢!)
```

**æ•™è®­**:
- Shared Memory é€‚åˆæœ‰æ•°æ®é‡ç”¨çš„åœºæ™¯
- ç›²ç›®åº”ç”¨ä¼˜åŒ–æŠ€æœ¯ä¼šé€‚å¾—å…¶å
- å¿…é¡»é€šè¿‡ NCU éªŒè¯å‡è®¾

### v4: Optimized - Final Tuning

**ç»¼åˆä¼˜åŒ–**:

1. **Loop Unrolling**:
```c
#pragma unroll 4
for (int i = idx; i < n; i += stride) {
    // ç¼–è¯‘å™¨å±•å¼€ 4 æ¬¡è¿­ä»£
    // å‡å°‘åˆ†æ”¯æŒ‡ä»¤
    // æå‡æŒ‡ä»¤è°ƒåº¦ç©ºé—´
}
```

2. **Pointer Hints**:
```c
const float4* __restrict__ a  // å‘Šè¯‰ç¼–è¯‘å™¨ a, b, c æ— åˆ«å
// å…è®¸æ›´æ¿€è¿›çš„ä¼˜åŒ–
```

3. **Optimal Grid Size**:
```c
int num_sms = 108;  // A100
int blocks = num_sms * 8;  // 8 blocks per SM
// æ¯ä¸ª SM è¿è¡Œå¤šä¸ª block â†’ éšè— latency
```

4. **Compilation**:
```bash
nvcc -O3 -use_fast_math -arch=sm_80 --maxrregcount=64
```

**NCU å®Œæ•´æŒ‡æ ‡**:
```
Speed of Light:
  Memory Throughput: 94.7%  ğŸ”´ (æ¥è¿‘æé™)
  SM Throughput: 19.8%

Memory Workload Analysis:
  l1tex__average_t_sectors_per_request: 1.0  âœ…
  L2 hit rate: 5.2% (streaming, æ­£å¸¸)
  DRAM throughput: 1416.7 GB/s  âœ…

Compute Workload Analysis:
  ILP (Inst per cycle): 2.8  ğŸŸ¢
  Warp execution efficiency: 100%  âœ…

Warp State Statistics:
  long_scoreboard: 42.3%  ğŸŸ¢ (å·²ä¼˜åŒ–)
  active: 45.7%  ğŸŸ¢
  barrier: 1.2%  âœ…

Occupancy:
  Theoretical: 100%
  Achieved: 98.7%  âœ…

Bandwidth: 1416.7 GB/s
Efficiency: 91.1%  âœ…
Speedup vs v2: 1.3x
Total speedup vs v0: 23.1x
```

**ç»“è®º**: å·²æ¥è¿‘ç¡¬ä»¶ç†è®ºæé™ï¼Œæ— éœ€è¿›ä¸€æ­¥ä¼˜åŒ–

## ğŸ“ˆ ä½•æ—¶åœæ­¢ä¼˜åŒ–

### Roofline åˆ¤æ–­

```python
# v4 æ€§èƒ½åˆ†æ
AI = 0.083 FLOPS/Byte (ä¸å˜)
Achieved BW = 1416.7 GB/s
Theoretical BW = 1555 GB/s
Efficiency = 91.1%

# åœæ­¢æ¡ä»¶:
if efficiency > 90%:
    print("å·²è¾¾ç¡¬ä»¶æé™ï¼Œåœæ­¢ä¼˜åŒ–å†…æ ¸")
    print("è½¬å‘:")
    print("  1. ç®—å­èåˆ (å‡å°‘ kernel launch)")
    print("  2. Pipeline ä¼˜åŒ– (overlap compute/transfer)")
    print("  3. å¤š GPU å¹¶è¡Œ")
```

### NCU éªŒè¯

```bash
# Speed of Light æ¥è¿‘ 100% â†’ å·²è¾¾æé™
Memory Throughput: 94.7%  âœ…
SM Throughput: 19.8%  (Memory-bound æ­£å¸¸)

# Warp Stall åˆç†åˆ†å¸ƒ
long_scoreboard: 42.3%  (Memory-bound ä¸å¯é¿å…)
active: 45.7%  (è‰¯å¥½)
barrier: 1.2%  (æœ€å°åŒ–)

# ç»“è®º: ç»§ç»­ä¼˜åŒ–å†…æ ¸æ”¶ç›Š < 10%ï¼Œä¸å€¼å¾—
```

### ROI è®¡ç®—

```python
def calculate_roi(current_time, potential_speedup, dev_days):
    """
    current_time: v4 æ—¶é—´ = 0.54 ms
    potential_speedup: å‡è®¾ä¼˜åŒ–åˆ° 1.5x = 0.36 ms
    dev_days: é¢„è®¡ 2 å¤©å¼€å‘
    """
    time_saved = current_time - (current_time / potential_speedup)
    # = 0.54 - 0.36 = 0.18 ms per call

    # å‡è®¾æ¯å¤©è¿è¡Œ 1M æ¬¡
    daily_saving = time_saved * 1e6 / 1000 / 3600  # hours
    # = 0.18 ms * 1M / 3600000 = 0.05 hours

    roi = daily_saving / dev_days
    # = 0.05 / 2 = 0.025 hours/day ROI

    return roi < 1.0  # Not worth it!

# ç»“è®º: v4 å·²ç»è¶³å¤Ÿå¥½ï¼Œè½¬å‘ç³»ç»Ÿçº§ä¼˜åŒ–
```

## ğŸ› ï¸ å·¥å…·å’Œè„šæœ¬

### Makefile ä½¿ç”¨

```bash
# ç¼–è¯‘æ‰€æœ‰ç‰ˆæœ¬
make all

# è¿è¡Œå•ä¸ªç‰ˆæœ¬
make v1_coalesced && ./v1_coalesced

# å®Œæ•´æ€§èƒ½å¯¹æ¯”
make compare

# NCU å®Œæ•´åˆ†æ (ç”Ÿæˆ .ncu-rep æ–‡ä»¶)
make ncu

# NCU å¿«é€ŸæŒ‡æ ‡
make ncu-quick

# æ¸…ç†
make clean

# å¸®åŠ©
make help
```

### benchmark.sh è„šæœ¬

è‡ªåŠ¨è¿è¡Œæ‰€æœ‰ç‰ˆæœ¬å¹¶ç”Ÿæˆå¯¹æ¯”è¡¨:

```bash
./benchmark.sh [problem_size]

# ä¾‹å¦‚: ä½¿ç”¨ 128M å…ƒç´  (512 MB per array)
./benchmark.sh 134217728

# è¾“å‡º: benchmark_results.txt
```

### NCU å‘½ä»¤å‚è€ƒ

```bash
# 1. å®Œæ•´åˆ†æ (æ‰€æœ‰æŒ‡æ ‡)
ncu --set full --export v4_optimized ./v4_optimized

# 2. å…³é”®æŒ‡æ ‡å¯¹æ¯”
ncu --metrics \
  gpu__time_duration.avg,\
  l1tex__average_t_sectors_per_request,\
  smsp__sass_inst_executed_op_global_ld,\
  dram__bytes.sum \
  ./v4_optimized

# 3. Memory ä¸“é¡¹åˆ†æ
ncu --set memory --export v1_coalesced ./v1_coalesced

# 4. å¯¹æ¯”ä¸¤ä¸ªç‰ˆæœ¬
ncu --set full --export v0_naive ./v0_naive
ncu --set full --export v4_optimized ./v4_optimized
# åœ¨ NCU-UI ä¸­åŒæ—¶æ‰“å¼€ä¸¤ä¸ª .ncu-rep è¿›è¡Œå¯¹æ¯”

# 5. å¯¼å‡º CSV æ‰¹é‡åˆ†æ
ncu --csv --metrics \
  l1tex__average_t_sectors_per_request,\
  smsp__average_warps_issue_stalled_long_scoreboard \
  ./v0_naive ./v1_coalesced ./v2_vectorized > comparison.csv
```

## ğŸ”— ç›¸å…³æ–‡æ¡£

- [CUDA/Triton ä¼˜åŒ–æŠ€æœ¯æ‰‹å†Œ](../../../techniques/cuda_triton_optimization_techniques.md) - 10 å¤§ä¼˜åŒ–æŠ€æœ¯è¯¦è§£
- [NCU UI ä½¿ç”¨æŒ‡å—](../../../04_performance_analysis/ncu_ui_guide.md) - NCU ç•Œé¢å®Œæ•´æ“ä½œ
- [NCU ä¸“å®¶çº§åˆ†æ](../../../04_performance_analysis/ncu_expert_analysis.md) - æ·±å…¥æŒ‡æ ‡è§£è¯»
- [å…¨å±€ä¼˜åŒ–æ¡†æ¶](../../../frameworks/global_optimization_framework.md) - 6 é˜¶æ®µä¼˜åŒ–æ–¹æ³•è®º
- [ä¼˜åŒ–å†³ç­–æŒ‡å—](../../../03_performance_optimization/optimization_decision_guide.md) - ä½•æ—¶ä½¿ç”¨ä½•ç§ä¼˜åŒ–

## ğŸ“ æ€»ç»“

### å…³é”®æ”¶è·

1. **Memory Coalescing æ˜¯åŸºç¡€**: å¿…é¡»å…ˆä¿®å¤è®¿é—®æ¨¡å¼
2. **Vectorization æœ‰æ™®éä»·å€¼**: å‡å°‘æŒ‡ä»¤ï¼Œæå‡ ILP
3. **ä¼˜åŒ–æŠ€æœ¯éœ€åŒ¹é…é—®é¢˜**: Shared Memory ä¸é€‚åˆæ— é‡ç”¨åœºæ™¯
4. **NCU æ˜¯éªŒè¯å·¥å…·**: ç”¨æ•°æ®é©±åŠ¨ä¼˜åŒ–å†³ç­–
5. **çŸ¥é“ä½•æ—¶åœæ­¢**: 90%+ æ•ˆç‡å·²æ˜¯æé™

### ä¼˜åŒ–æµç¨‹æ€»ç»“

```
1. Baseline (v0)
   â†“ NCU è¯Šæ–­: sectors_per_request = 32

2. Fix Coalescing (v1) â†’ 8x
   â†“ NCU è¯Šæ–­: æŒ‡ä»¤æ•°è¿‡å¤š

3. Vectorization (v2) â†’ 2x
   â†“ å°è¯•: Shared Memory (v3) â†’ âŒ å˜æ…¢
   â†“ NCU è¯Šæ–­: barrier stall å¢åŠ 

4. Fine-tuning (v4) â†’ 1.5x
   â†“ NCU éªŒè¯: 91% æ•ˆç‡

5. åœæ­¢å†…æ ¸ä¼˜åŒ–
   â†’ è½¬å‘ç³»ç»Ÿçº§ä¼˜åŒ–
```

### ä¸‹ä¸€æ­¥

- å­¦ä¹  GEMM ä¼˜åŒ–: [GEMM æ¡ˆä¾‹](../../matrix_ops/gemm/)
- äº†è§£ç®—å­èåˆ: [Kernel Fusion](../fused_ops/)
- ç³»ç»Ÿçº§ä¼˜åŒ–: [Global Optimization Framework](../../../frameworks/global_optimization_framework.md)

---

**é—®é¢˜åé¦ˆ**: å¦‚æœ‰ç–‘é—®ï¼Œå‚è€ƒ [PROJECT_REDESIGN.md](../../../PROJECT_REDESIGN.md) äº†è§£é¡¹ç›®æ•´ä½“è®¾è®¡
