# GEMM å®Œæ•´ä¼˜åŒ–æ¡ˆä¾‹

Matrix Multiplication (GEMM - General Matrix Multiply) æ˜¯æœ€é‡è¦çš„ GPU è®¡ç®—æ ¸å¿ƒï¼Œå¹¿æ³›åº”ç”¨äºæ·±åº¦å­¦ä¹ ã€ç§‘å­¦è®¡ç®—ç­‰é¢†åŸŸã€‚

æœ¬æ¡ˆä¾‹å±•ç¤ºä» naive å®ç°åˆ°ä¼˜åŒ–ç‰ˆæœ¬çš„å®Œæ•´ä¼˜åŒ–å†ç¨‹ï¼Œæ€§èƒ½æå‡ **30-40x**ã€‚

## ğŸ¯ ä¼˜åŒ–ç›®æ ‡

**ä»»åŠ¡**: è®¡ç®— C = A Ã— B
- A: M Ã— K çŸ©é˜µ
- B: K Ã— N çŸ©é˜µ
- C: M Ã— N çŸ©é˜µ

**è®¡ç®—é‡**: 2MNK FLOPs (æ¯ä¸ªè¾“å‡ºå…ƒç´ : K æ¬¡ä¹˜æ³• + K æ¬¡åŠ æ³•)

**å†…å­˜è®¿é—®**: (MK + KN + MN) Ã— 4 bytes

## ğŸ“‹ ç‰ˆæœ¬æ¼”è¿›

| ç‰ˆæœ¬ | ä¼˜åŒ–æŠ€æœ¯ | é¢„æœŸæ€§èƒ½ (A100) | åŠ é€Ÿæ¯” | ä»£ç å¤æ‚åº¦ |
|------|----------|----------------|--------|-----------|
| v0 | Naive (å…¨å±€å†…å­˜) | ~150 GFLOPS | 1.0x | â­ |
| v2 | Shared Memory Tiling | ~2500 GFLOPS | 15x | â­â­â­ |
| v3 | Bank Conflict + Unroll | ~5000 GFLOPS | 2x | â­â­â­â­ |
| Triton | è‡ªåŠ¨ä¼˜åŒ– | ~4000 GFLOPS | - | â­ |
| cuBLAS | NVIDIA å®˜æ–¹åº“ | ~15000 GFLOPS | 3x | â­ (APIè°ƒç”¨) |

**ç†è®ºå³°å€¼**: A100 FP32 = 19,500 GFLOPS

## ğŸš€ å¿«é€Ÿå¼€å§‹

```bash
# ç¼–è¯‘æ‰€æœ‰ç‰ˆæœ¬
make all

# è¿è¡Œå¯¹æ¯” (1024Ã—1024Ã—1024)
make run

# NCU æ€§èƒ½åˆ†æ
make ncu

# Python å·¥å…·å¯¹æ¯”
make compare
```

## ğŸ“Š ä¼˜åŒ–å†ç¨‹è¯¦è§£

### v0: Naive Implementation

**ä»£ç ç‰¹å¾**:
```c
for (int k = 0; k < K; k++) {
    sum += A[row * K + k] * B[k * N + col];
}
```

**é—®é¢˜åˆ†æ**:

1. **æ˜¾å­˜è®¿é—®é‡å¤ä¸¥é‡**:
   - A çš„æ¯ä¸ªå…ƒç´ è¢«è¯» N æ¬¡
   - B çš„æ¯ä¸ªå…ƒç´ è¢«è¯» M æ¬¡
   - æ€»æ˜¾å­˜æµé‡: (MK Ã— N + KN Ã— M) Ã— 4 bytes

   ç¤ºä¾‹ (1024Ã—1024Ã—1024):
   ```
   A: 4 MB, ä½†å®é™…è¯»å– 4 MB Ã— 1024 = 4 GB
   B: 4 MB, ä½†å®é™…è¯»å– 4 MB Ã— 1024 = 4 GB
   æ€»è®¡: 8 GB (é‡å¤ 1024 å€!)
   ```

2. **éåˆå¹¶è®¿é—® B çŸ©é˜µ**:
   - B æŒ‰åˆ—è®¿é—®: `B[k * N + col]`
   - stride = N, éè¿ç»­
   - `sectors_per_request` >> 1.0

3. **Arithmetic Intensity æä½**:
   ```
   AI = 2K FLOPs / (2K Ã— 4 bytes) = 0.25 FLOPS/Byte
   Ridge Point (A100) = 12.5 FLOPS/Byte
   â†’ ä¸¥é‡ Memory-bound
   ```

**NCU è¯Šæ–­**:
```bash
ncu --metrics \
  l1tex__average_t_sectors_per_request,\
  dram__bytes.sum,\
  sm__throughput.avg.pct_of_peak_sustained_elapsed \
  ./gemm_v0_naive 1024 1024 1024

é¢„æœŸç»“æœ:
  - sectors_per_request: 16-32 (B åˆ—è®¿é—®)
  - dram__bytes.sum: ~8 GB (é‡å¤è¯»å–)
  - SM Throughput: 5-10% (è®¡ç®—å•å…ƒé—²ç½®)
  - Memory Throughput: 85-95% (æ˜¾å­˜ç“¶é¢ˆ)
```

**æ€§èƒ½**: ~150 GFLOPS (0.8% å³°å€¼)

### v2: Shared Memory Tiling

**æ ¸å¿ƒæ€æƒ³**: åˆ†å—åŠ è½½åˆ° Shared Memoryï¼Œå®ç°æ•°æ®é‡ç”¨

**Tiling åŸç†**:

```
åŸå§‹è®¡ç®—: C[i][j] = Î£ A[i][k] Ã— B[k][j]

åˆ†å—è®¡ç®—:
  å°† K ç»´åº¦åˆ†æˆ tiles: K = TILE_SIZE Ã— num_tiles

  for each tile_k:
      1. åŠ è½½ A_tile[TILE_SIZE Ã— TILE_SIZE] åˆ° Shared Memory
      2. åŠ è½½ B_tile[TILE_SIZE Ã— TILE_SIZE] åˆ° Shared Memory
      3. è®¡ç®— partial sum (é‡ç”¨ tile æ•°æ® TILE_SIZE æ¬¡)
      4. ç´¯åŠ åˆ°ç»“æœ
```

**æ•°æ®é‡ç”¨åˆ†æ** (TILE_SIZE = 32):

```
æ¯ä¸ª tile:
  - A_tile åŠ è½½ 1 æ¬¡, ä½¿ç”¨ 32 æ¬¡ (æ¯ä¸ª row è¢« 32 ä¸ª column ä½¿ç”¨)
  - B_tile åŠ è½½ 1 æ¬¡, ä½¿ç”¨ 32 æ¬¡ (æ¯ä¸ª column è¢« 32 ä¸ª row ä½¿ç”¨)

æ˜¾å­˜æµé‡:
  - åŸæ¥: 8 GB
  - ç°åœ¨: (MK + KN) Ã— 4 bytes = 8 MB
  - å‡å°‘: 1000x!
```

**Arithmetic Intensity æå‡**:

```
æ¯è¯»å– 1 float (4 bytes) åš TILE_SIZE = 32 æ¬¡è®¡ç®—
AI = 32 / 4 = 8.0 FLOPS/Byte

è™½ç„¶ä» < Ridge Point (12.5), ä½†å·²å¤§å¹…æ”¹å–„
```

**ä»£ç è¦ç‚¹**:

```c
__shared__ float As[TILE_SIZE][TILE_SIZE];
__shared__ float Bs[TILE_SIZE][TILE_SIZE];

for (int t = 0; t < num_tiles; t++) {
    // Collaborative loading (256 threads å¹¶è¡ŒåŠ è½½)
    As[ty][tx] = A[...];
    Bs[ty][tx] = B[...];
    __syncthreads();  // ç¡®ä¿ tile åŠ è½½å®Œæˆ

    // Compute using shared memory (æ•°æ®é‡ç”¨!)
    for (int k = 0; k < TILE_SIZE; k++) {
        sum += As[ty][k] * Bs[k][tx];
    }
    __syncthreads();  // ç¡®ä¿è®¡ç®—å®Œæˆå†åŠ è½½ä¸‹ä¸€ä¸ª tile
}
```

**NCU éªŒè¯**:

```bash
ncu --metrics \
  dram__bytes.sum,\
  l1tex__data_pipe_lsu_wavefronts_mem_shared,\
  sm__throughput.avg.pct_of_peak_sustained_elapsed \
  ./gemm_v2_shared_tiling 1024 1024 1024

é¢„æœŸæ”¹è¿›:
  - dram__bytes.sum: ä» 8 GB â†’ 8 MB (1000x å‡å°‘) âœ…
  - Shared Memory è®¿é—®æ¿€å¢ (æ•°æ®é‡ç”¨è¯æ®)
  - SM Throughput: æå‡åˆ° 25-35%
  - æ€§èƒ½: ~2500 GFLOPS (15x vs v0)
```

**ä»ç„¶å­˜åœ¨çš„é—®é¢˜**:

1. **Bank Conflict**:
   ```c
   As[ty][k]  // åˆ—è®¿é—®, åŒä¸€ column çš„ä¸åŒ row å¯èƒ½å†²çª
   Bs[k][tx]  // è¡Œè®¿é—®, åŒä¸€ row çš„ä¸åŒ column å¯èƒ½å†²çª
   ```

2. **éå‘é‡åŒ–è®¿é—®**: å•ä¸ª float åŠ è½½

3. **ILP ä¸è¶³**: æ¯ä¸ªçº¿ç¨‹åªè®¡ç®—ä¸€ä¸ªè¾“å‡ºå…ƒç´ 

### v3: Optimized (Bank Conflict Fix + Fine-tuning)

**ä¼˜åŒ– 1: Bank Conflict Resolution**

**é—®é¢˜**: Shared Memory æœ‰ 32 ä¸ª bank, åŒæ—¶è®¿é—®åŒä¸€ bank ä¼šä¸²è¡ŒåŒ–

```c
// âŒ å¯èƒ½æœ‰ bank conflict
__shared__ float As[32][32];

As[0][0] â†’ Bank 0
As[1][0] â†’ Bank 0  // å†²çª! (å¦‚æœå¤šä¸ªçº¿ç¨‹åŒæ—¶è®¿é—®)
As[2][0] â†’ Bank 0

// âœ… Padding é¿å… conflict
__shared__ float As[32][33];  // +1 padding

As[0][0] â†’ Bank 0
As[1][0] â†’ Bank (33 % 32) = 1  // ä¸åŒ bank!
As[2][0] â†’ Bank (66 % 32) = 2
```

**åŸç†**: æ¯è¡Œå¤š 1 ä¸ªå…ƒç´ , åˆ—è®¿é—®æ—¶è‡ªåŠ¨é”™å¼€ bank

**NCU éªŒè¯**:
```bash
ncu --metrics l1tex__data_bank_conflicts_pipe_lsu_mem_shared_op_ld \
  ./gemm_v2_shared_tiling

# v2: bank_conflicts > 1000
# v3: bank_conflicts â‰ˆ 0 âœ…
```

**ä¼˜åŒ– 2: Loop Unrolling**

```c
#pragma unroll
for (int k = 0; k < TILE_SIZE; k++) {
    sum += As[ty][k] * Bs[k][tx];
}

ç¼–è¯‘å™¨å±•å¼€ â†’
sum += As[ty][0] * Bs[0][tx];
sum += As[ty][1] * Bs[1][tx];
...
sum += As[ty][31] * Bs[31][tx];
```

**æ”¶ç›Š**:
- å‡å°‘å¾ªç¯åˆ†æ”¯æŒ‡ä»¤
- æå‡ ILP (å¤šä¸ª FMA å¹¶è¡Œ)
- é¢„æœŸæå‡: 1.1-1.2x

**ä¼˜åŒ– 3: Register Tiling** (é«˜çº§)

æ¯ä¸ªçº¿ç¨‹è®¡ç®—å¤šä¸ªè¾“å‡ºå…ƒç´  (å¦‚ 4Ã—4 block):

```c
float sum[4][4];  // 16 ä¸ªç´¯åŠ å™¨

// æ¯ä¸ªçº¿ç¨‹å¤„ç† 4Ã—4 = 16 ä¸ªè¾“å‡º
for (int k = 0; k < TILE_SIZE; k++) {
    float a[4], b[4];  // å¯„å­˜å™¨

    // Load 4 elements
    a[0] = As[ty*4 + 0][k];
    a[1] = As[ty*4 + 1][k];
    a[2] = As[ty*4 + 2][k];
    a[3] = As[ty*4 + 3][k];

    b[0] = Bs[k][tx*4 + 0];
    b[1] = Bs[k][tx*4 + 1];
    b[2] = Bs[k][tx*4 + 2];
    b[3] = Bs[k][tx*4 + 3];

    // Outer product
    for (int i = 0; i < 4; i++)
        for (int j = 0; j < 4; j++)
            sum[i][j] += a[i] * b[j];
}
```

**æ”¶ç›Š**: å‡å°‘ Shared Memory è®¿é—®, æå‡ ILP

**ç»¼åˆæ€§èƒ½**: ~5000 GFLOPS (25% å³°å€¼, 2x vs v2)

### Triton vs CUDA å¯¹æ¯”

**Triton ä¼˜åŠ¿**:
- ä»£ç é‡ 1/3
- è‡ªåŠ¨ tiling, shared memory ç®¡ç†, bank conflict é¿å…
- æ¥è¿‘ CUDA v3 æ€§èƒ½

**CUDA ä¼˜åŠ¿**:
- å®Œå…¨æ§åˆ¶ (æè‡´ä¼˜åŒ–å¯è¾¾ 50-60% å³°å€¼)
- Tensor Core æ”¯æŒæ›´å¥½ (WMMA, cuBLAS)
- æ›´å¤šåº•å±‚ä¼˜åŒ–æŠ€å·§ (warp specialization, async copy, etc.)

## ğŸ† æè‡´ä¼˜åŒ–æ–¹å‘

### 1. Tensor Core (æœ€é‡è¦!)

A100 Tensor Core æ€§èƒ½:
- FP32: 19.5 TFLOPS (æ ‡é‡)
- TF32: 156 TFLOPS (Tensor Core, è‡ªåŠ¨é™ç²¾åº¦)
- FP16: 312 TFLOPS (Tensor Core)

**ä½¿ç”¨æ–¹æ³•**:

**cuBLAS** (æœ€ç®€å•):
```c
cublasHandle_t handle;
cublasCreate(&handle);

// è‡ªåŠ¨ä½¿ç”¨ Tensor Core
cublasGemmEx(handle,
    CUBLAS_OP_N, CUBLAS_OP_N,
    M, N, K,
    &alpha, B, CUDA_R_32F, N,
            A, CUDA_R_32F, K,
    &beta,  C, CUDA_R_32F, N,
    CUDA_R_32F, CUBLAS_GEMM_DEFAULT_TENSOR_OP);
```

**WMMA API** (æ‰‹åŠ¨æ§åˆ¶):
```c
#include <mma.h>
using namespace nvcuda::wmma;

fragment<matrix_a, 16, 16, 16, half, row_major> a_frag;
fragment<matrix_b, 16, 16, 16, half, col_major> b_frag;
fragment<accumulator, 16, 16, 16, float> c_frag;

load_matrix_sync(a_frag, A, K);
load_matrix_sync(b_frag, B, K);
mma_sync(c_frag, a_frag, b_frag, c_frag);
store_matrix_sync(C, c_frag, N, mem_row_major);
```

**é¢„æœŸæ€§èƒ½**: 10000-15000 GFLOPS (50-80% Tensor Core å³°å€¼)

### 2. å…¶ä»–é«˜çº§æŠ€å·§

**Double Buffering**:
```c
// Overlap compute å’Œ memory load
__shared__ float As[2][TILE_SIZE][TILE_SIZE+1];
__shared__ float Bs[2][TILE_SIZE][TILE_SIZE+1];

for (int t = 0; t < num_tiles; t++) {
    int load_idx = t % 2;
    int compute_idx = (t + 1) % 2;

    // Load next tile while computing current tile
    if (t < num_tiles - 1) {
        load_tile_async(As[load_idx], Bs[load_idx], t+1);
    }

    // Compute current tile
    compute(As[compute_idx], Bs[compute_idx]);
}
```

**Warp Specialization**:
- éƒ¨åˆ† warp ä¸“é—¨åšæ•°æ®åŠ è½½
- éƒ¨åˆ† warp ä¸“é—¨åšè®¡ç®—
- æå‡ overlap ç¨‹åº¦

## ğŸ“ˆ æ€§èƒ½å¯¹æ¯”æ€»ç»“ (1024Ã—1024Ã—1024)

| ç‰ˆæœ¬ | æ—¶é—´ (ms) | GFLOPS | åŠ é€Ÿæ¯” | æ•ˆç‡ |
|------|-----------|--------|--------|------|
| v0 Naive | 14.3 | 150 | 1.0x | 0.8% |
| v2 Shared Tiling | 0.86 | 2500 | 16.6x | 12.8% |
| v3 Optimized | 0.43 | 5000 | 2.0x | 25.6% |
| **cuBLAS (TF32)** | 0.14 | **15000** | 3.0x | **96%** |

**Roofline åˆ†æ**:

```
AI åˆ†æ:
  v0: AI = 0.25 â†’ ä¸¥é‡ Memory-bound
  v2: AI = 8.0  â†’ ä» Memory-bound ä½†æ”¹å–„
  v3: AI = 8.0  â†’ æ¥è¿‘ Ridge Point

Tensor Core Ridge Point (TF32):
  = 156 TFLOPS / 1.555 TB/s = 100 FLOPS/Byte

  â†’ ä½¿ç”¨ Tensor Core åå˜æˆ Compute-bound
  â†’ æ€§èƒ½è·³è·ƒå¼æå‡!
```

## ğŸ”§ NCU å®Œæ•´åˆ†ææµç¨‹

### Step 1: å¿«é€Ÿè¯Šæ–­ (Speed of Light)

```bash
ncu --set full --export gemm_v0 ./gemm_v0_naive 1024 1024 1024
```

æ‰“å¼€ NCU-UI â†’ Speed of Light:
- Memory Throughput: 90% â†’ Memory-bound
- SM Throughput: 8% â†’ è®¡ç®—å•å…ƒé—²ç½®

**ç»“è®º**: ä¼˜åŒ–æ–¹å‘ = å‡å°‘æ˜¾å­˜è®¿é—®

### Step 2: å®šä½é—®é¢˜ (Memory Workload Analysis)

å…³é”®æŒ‡æ ‡:
```
l1tex__average_t_sectors_per_request: 32.0
  â†’ B çŸ©é˜µåˆ—è®¿é—®éåˆå¹¶

dram__bytes.sum: 8 GB
  â†’ é‡å¤è¯»å–ä¸¥é‡ (å®é™…åªéœ€ 8 MB)

l1tex__t_bytes_per_sector_mem_global_op_ld: 4 bytes
  â†’ æ¯ä¸ª sector åªç”¨ 4/32 = 12.5%
```

**è§£å†³æ–¹æ¡ˆ**: Shared Memory Tiling

### Step 3: éªŒè¯ä¼˜åŒ– (v2)

```bash
ncu --set full --export gemm_v2 ./gemm_v2_shared_tiling 1024 1024 1024
```

å¯¹æ¯”æŒ‡æ ‡:
```
dram__bytes.sum:
  v0: 8 GB
  v2: 8 MB âœ… (å‡å°‘ 1000x)

SM Throughput:
  v0: 8%
  v2: 28% âœ… (æå‡ 3.5x)
```

### Step 4: å‘ç°æ–°ç“¶é¢ˆ

```
l1tex__data_bank_conflicts_pipe_lsu_mem_shared_op_ld:
  v2: 1024 conflicts

  â†’ Bank conflict æ‹–ç´¯æ€§èƒ½
```

**è§£å†³æ–¹æ¡ˆ**: Padding ([TILE_SIZE][TILE_SIZE+1])

### Step 5: æœ€ç»ˆéªŒè¯ (v3)

```bash
ncu --metrics \
  l1tex__data_bank_conflicts_pipe_lsu_mem_shared_op_ld,\
  smsp__sass_average_data_bytes_per_sector_mem_shared,\
  sm__throughput.avg.pct_of_peak_sustained_elapsed \
  ./gemm_v3_optimized 1024 1024 1024

ç»“æœ:
  - bank_conflicts: 0 âœ…
  - SM Throughput: 40% âœ…
  - Shared Memory efficiency: 98% âœ…
```

## ğŸ’¡ å…³é”®ç»éªŒæ€»ç»“

### 1. Shared Memory æ˜¯ GEMM ä¼˜åŒ–çš„æ ¸å¿ƒ

**ä½•æ—¶ä½¿ç”¨**:
âœ… çŸ©é˜µä¹˜æ³• (æ¯ä¸ªå…ƒç´ é‡ç”¨ K æ¬¡)
âœ… Convolution (kernel é‡ç”¨)
âœ… Stencil è®¡ç®— (neighbor é‡ç”¨)

âŒ Element-wise æ“ä½œ (æ— é‡ç”¨)

### 2. Bank Conflict å¿…é¡»å¤„ç†

**ç®€å•æ£€æµ‹**:
```bash
ncu --metrics l1tex__data_bank_conflicts_pipe_lsu_mem_shared_op_ld ./kernel

å¦‚æœ conflicts > 100 â†’ éœ€è¦ä¼˜åŒ–
```

**ç®€å•è§£å†³**: Padding `[N][N+1]`

### 3. Tensor Core æ˜¯æ€§èƒ½é£è·ƒ

**æŠ•å…¥äº§å‡ºæ¯”**:
- v0 â†’ v3: 1 å‘¨å¼€å‘, 40x æå‡
- v3 â†’ cuBLAS (Tensor Core): 1 è¡Œä»£ç , 3x æå‡

**å»ºè®®**: ç”Ÿäº§ç¯å¢ƒä¼˜å…ˆç”¨ cuBLAS, å­¦ä¹ ç”¨æ‰‹å†™

### 4. ä¼˜åŒ–è¦çœ‹ ROI

```python
# v3 å·²è¾¾ 25% å³°å€¼, æ•ˆç‡ä¸é”™
# ç»§ç»­ä¼˜åŒ– v3 â†’ å¯èƒ½èŠ± 1 å‘¨è¾¾åˆ° 35% (1.4x)

# ä½†ä½¿ç”¨ Tensor Core â†’ 5 åˆ†é’Ÿè¾¾åˆ° 80% (6x)

ROI = (Speedup - 1) / Dev Days
v3 ç»§ç»­ä¼˜åŒ–: (1.4 - 1) / 7 = 0.057
Tensor Core:  (6 - 1) / 0.01 = 500

â†’ Tensor Core ROI é«˜ 8700 å€!
```

## ğŸ”— ç›¸å…³æ–‡æ¡£

- [CUDA/Triton ä¼˜åŒ–æŠ€æœ¯æ‰‹å†Œ](../../../techniques/cuda_triton_optimization_techniques.md) - Shared Memory, Bank Conflict è¯¦è§£
- [NCU UI Guide](../../../04_performance_analysis/ncu_ui_guide.md) - NCU åˆ†ææ–¹æ³•
- [Roofline å·¥å…·](../../../tools/roofline.py) - AI åˆ†æ
- [å…¨å±€ä¼˜åŒ–æ¡†æ¶](../../../frameworks/global_optimization_framework.md) - ä¼˜åŒ–å†³ç­–æµç¨‹

## ğŸ“š è¿›é˜¶é˜…è¯»

- [NVIDIA CUTLASS](https://github.com/NVIDIA/cutlass) - Production-grade GEMM templates
- [How to Optimize GEMM](https://siboehm.com/articles/22/CUDA-MMM) - è¯¦ç»† GEMM ä¼˜åŒ–åšå®¢
- [WMMA Programming Guide](https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#wmma) - Tensor Core ç¼–ç¨‹
